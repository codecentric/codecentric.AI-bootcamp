{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "# NLP Basics: Wer redet da?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "_Hallo und herzlich willkommen zum codecentric.ai bootcamp!_\n",
    "\n",
    "In diesem Tutorial beschäftigen wir uns mit Grundlagen der automatischen Textanalyse (NLP). Dazu haben wir einen Datensatz mit Reden deutscher Politiker ausgesucht und wollen ein Modell trainieren, das zu einer gegebenen Rede herausbekommt, wer sie gehalten hat! Trainiert wird das Modell mit Reden, zu denen es den Redner vorgesagt bekommt. Gemeinsam werden wir\n",
    "\n",
    "1. den Datensatz von der Quelle herunterladen, inspizieren und bereinigen,\n",
    "2. die Reden mit Hilfe der NLP-Bibliothek [spaCy](https://spacy.io) vorverarbeiten: tokenisieren, lemmatisieren und Begriffe extrahieren,\n",
    "3. statistische Informationen wie _bag of words_ extrahieren, um jede Rede durch einen Vektor darzustellen, der als Eingabe für ein Klassifikationsmodell genutzt werden kann,\n",
    "4. mit Hilfe der _machine learning_-Bibliothek [scikit-learn](https://sklearn.org) und der _deep learning_-Bibliothek [keras](https://keras.io) verschiedene Modelle zur Klassifikation trainieren und\n",
    "5. abschließend als Vorbereitung für einen tieferen Einstieg Worteinbettungen betrachten und für die Klassifikation der Reden verwenden.\n",
    "\n",
    "Eine Einführung in die Grundlagen von NLP gibt [dieses YouTube-Video](https://www.youtube.com/watch?v=GmLsb-o7hvM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"850\"\n",
       "            height=\"650\"\n",
       "            src=\"https://www.youtube.com/embed/GmLsb-o7hvM\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f5e3c239860>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lade Video\n",
    "from IPython.display import IFrame    \n",
    "IFrame('https://www.youtube.com/embed/GmLsb-o7hvM', width=850, height=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Der [folgende Screencast](https://www.youtube.com/watch?v=Vf4WdacLUiY&feature=youtu.be) gibt einen kurzen Überblick über das Notebook.\n",
    "\n",
    "_Viel Spaß!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"850\"\n",
       "            height=\"650\"\n",
       "            src=\"https://www.youtube.com/embed/Vf4WdacLUiY\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f5e3c24fef0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lade Video\n",
    "from IPython.display import IFrame    \n",
    "IFrame('https://www.youtube.com/embed/Vf4WdacLUiY', width=850, height=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Datensatz einlesen, betrachten und bereinigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als Datensatz nutzen wir eine Sammlung von Reden, die Mitglieder der Bundesregierung beziehungsweise Bundespräsidenten gehalten haben, und die von Adrien Barbaresi zusammengestellt wurde: \n",
    "\n",
    "> \\[1\\] Barbaresi, Adrien (2018). _A corpus of German political speeches from the 21st century._ Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), European Language Resources Association (ELRA), pp. 792–797.\n",
    "\n",
    "Da diese Sammlung sehr umfangreich ist, haben wir im Verzeichnis `data` einen kleineren Auszug vorbereitet und \n",
    "\n",
    "- die Anzahl an Reden pro Redner auf 50 beschränkt,\n",
    "- die Reden ab dem 1000ten Zeichen abgeschnitten.\n",
    "\n",
    "Die folgende Funktion liest per default den reduzierten Auszug in einen [pandas](https://pandas.pydata.org)-DataFrame ein, lädt auf Wunsch aber auch den gesamten Datensatz herunter. Letzteres kann eine ganze Weile dauern und ist für dieses Tutorial nicht nötig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import zipfile\n",
    "import xmltodict\n",
    "\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "DATA_FILE = \"speeches.json\"\n",
    "REMOTE_PATH = \"http://adrien.barbaresi.eu/corpora/speeches/\"\n",
    "REMOTE_FILE = \"German-political-speeches-2018-release.zip\"\n",
    "REMOTE_DATASET = \"Bundesregierung.xml\"\n",
    "REMOTE_URL = REMOTE_PATH + REMOTE_FILE\n",
    "\n",
    "\n",
    "def download_and_extract_data():\n",
    "    zip_path = os.path.join(DATA_PATH, REMOTE_FILE)\n",
    "    urllib.request.urlretrieve(REMOTE_URL, zip_path)\n",
    "    with zipfile.ZipFile(zip_path) as file:\n",
    "        file.extract(REMOTE_DATASET, path = DATA_PATH)\n",
    "\n",
    "\n",
    "def load_data(reduced=True):\n",
    "    if reduced:\n",
    "        return pd.read_json(os.path.join(DATA_PATH, DATA_FILE))\n",
    "    else:\n",
    "        zip_path = os.path.join(DATA_PATH, REMOTE_FILE)\n",
    "        if not os.path.isfile(zip_path):\n",
    "            download_and_extract_data()\n",
    "        file = os.path.join(DATA_PATH, REMOTE_DATASET) \n",
    "        with open(file, mode=\"rb\") as file:\n",
    "            xml_document = xmltodict.parse(file)\n",
    "            text_nodes = xml_document['collection']['text']\n",
    "            persons = [t['@person'] for t in text_nodes]\n",
    "            speeches = [t['rohtext'] for t in text_nodes]\n",
    "            return pd.DataFrame({'person' : persons, 'speech' : speeches})\n",
    "        \n",
    "        \n",
    "df = load_data(reduced=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Unser DataFrame `df` ist eine Tabelle mit zwei Spalten: `person` und `speech`. Jede Zeile enthält also in der ersten Spalte einen Redner und in der zweiten eine Rede, beides jeweils als Python-String. Schauen wir uns den DataFrame genauer an: die Methode\n",
    "- `head()` zeigt uns die ersten Zeilen der Tabelle,\n",
    "- `value_counts()` zählt für jeden Redner, wie oft er in der Spalte auftaucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andere</td>\n",
       "      <td>Kulturstaatsministerin Grütters hat beim Empfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andere</td>\n",
       "      <td>Unter dem Titel \"Verteidigt die Kultur!\" hat d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Angela Merkel</td>\n",
       "      <td>Liebe Kolleginnen und Kollegen aus dem Deutsch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Bernd Neumann</td>\n",
       "      <td>In seiner Rede betonte Staatsminister Bernd Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Bernd Neumann</td>\n",
       "      <td>dass ein überaus erfolgreicher Rockmusiker im ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Bernd Neumann</td>\n",
       "      <td>ich freue mich sehr, heute gemeinsam mit Ihnen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Bernd Neumann</td>\n",
       "      <td>es war eine wichtige und richtungweisende Ents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Christina Weiss</td>\n",
       "      <td>Als vor vier Jahren das Amt des oder nunmehr d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Christina Weiss</td>\n",
       "      <td>--\\nauf diesem Festival wird ein deutscher Fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Christina Weiss</td>\n",
       "      <td>Anlässlich der heutigen Pressekonferenz von \"Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              person                                             speech\n",
       "0             Andere  Kulturstaatsministerin Grütters hat beim Empfa...\n",
       "1             Andere  Unter dem Titel \"Verteidigt die Kultur!\" hat d...\n",
       "10     Angela Merkel  Liebe Kolleginnen und Kollegen aus dem Deutsch...\n",
       "100    Bernd Neumann  In seiner Rede betonte Staatsminister Bernd Ne...\n",
       "101    Bernd Neumann  dass ein überaus erfolgreicher Rockmusiker im ...\n",
       "102    Bernd Neumann  ich freue mich sehr, heute gemeinsam mit Ihnen...\n",
       "103    Bernd Neumann  es war eine wichtige und richtungweisende Ents...\n",
       "104  Christina Weiss  Als vor vier Jahren das Amt des oder nunmehr d...\n",
       "105  Christina Weiss  --\\nauf diesem Festival wird ein deutscher Fil...\n",
       "106  Christina Weiss  Anlässlich der heutigen Pressekonferenz von \"Y..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Michael Naumann            50\n",
       "Bernd Neumann              50\n",
       "Gerhard Schröder           50\n",
       "Angela Merkel              50\n",
       "Monika Grütters            50\n",
       "Christina Weiss            50\n",
       "k.A.                       50\n",
       "Julian Nida-Rümelin        47\n",
       "Thomas de Maizière         43\n",
       "Hans Martin Bury           42\n",
       "Joschka Fischer            31\n",
       "Rolf Schwanitz             24\n",
       "Frank-Walter Steinmeier     7\n",
       "Andere                      4\n",
       "Jullian Nida-Rümelin        1\n",
       "Name: person, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.person.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir sehen, dass der Datensatz &mdash; wie so oft in der Praxis &mdash; Fehler beziehungsweise Unvollständigkeiten enthält:\n",
    "\n",
    "- Julian Nida-Rümelins Name wurde einmal falsch geschrieben;\n",
    "- für einige Reden wurde kein Redner angegeben, sondern \"k.A.\" oder \"Andere\". \n",
    "\n",
    "Wir korrigieren den Namen und entfernen alle Reden ohne Redner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Michael Naumann            50\n",
       "Bernd Neumann              50\n",
       "Gerhard Schröder           50\n",
       "Angela Merkel              50\n",
       "Monika Grütters            50\n",
       "Christina Weiss            50\n",
       "Julian Nida-Rümelin        48\n",
       "Thomas de Maizière         43\n",
       "Hans Martin Bury           42\n",
       "Joschka Fischer            31\n",
       "Rolf Schwanitz             24\n",
       "Frank-Walter Steinmeier     7\n",
       "Name: person, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['person'] = df.person.str.replace(\"Jullian Nida-Rümelin\", \"Julian Nida-Rümelin\")\n",
    "df = df.query(\"person not in ['k.A.', 'Andere']\")\n",
    "df.person.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Diese Werte können wir wie folgt visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"267.104062pt\" version=\"1.1\" viewBox=\"0 0 499.262656 267.104062\" width=\"499.262656pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 267.104062 \n",
       "L 499.262656 267.104062 \n",
       "L 499.262656 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 151.262656 239.758125 \n",
       "L 486.062656 239.758125 \n",
       "L 486.062656 22.318125 \n",
       "L 151.262656 22.318125 \n",
       "z\n",
       "\" style=\"fill:#eaeaf2;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 239.758125 \n",
       "L 151.262656 22.318125 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(147.763281 257.616406)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 215.034085 239.758125 \n",
       "L 215.034085 22.318125 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 10 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(208.035335 257.616406)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 278.805513 239.758125 \n",
       "L 278.805513 22.318125 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 20 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(271.806763 257.616406)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 342.576942 239.758125 \n",
       "L 342.576942 22.318125 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 30 -->\n",
       "      <defs>\n",
       "       <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(335.578192 257.616406)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 406.348371 239.758125 \n",
       "L 406.348371 22.318125 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 40 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(399.349621 257.616406)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 470.119799 239.758125 \n",
       "L 470.119799 22.318125 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 50 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(463.121049 257.616406)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"text_7\">\n",
       "      <!-- Angela Merkel -->\n",
       "      <defs>\n",
       "       <path d=\"M 34.1875 63.1875 \n",
       "L 20.796875 26.90625 \n",
       "L 47.609375 26.90625 \n",
       "z\n",
       "M 28.609375 72.90625 \n",
       "L 39.796875 72.90625 \n",
       "L 67.578125 0 \n",
       "L 57.328125 0 \n",
       "L 50.6875 18.703125 \n",
       "L 17.828125 18.703125 \n",
       "L 11.1875 0 \n",
       "L 0.78125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-65\"/>\n",
       "       <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "       <path d=\"M 45.40625 27.984375 \n",
       "Q 45.40625 37.75 41.375 43.109375 \n",
       "Q 37.359375 48.484375 30.078125 48.484375 \n",
       "Q 22.859375 48.484375 18.828125 43.109375 \n",
       "Q 14.796875 37.75 14.796875 27.984375 \n",
       "Q 14.796875 18.265625 18.828125 12.890625 \n",
       "Q 22.859375 7.515625 30.078125 7.515625 \n",
       "Q 37.359375 7.515625 41.375 12.890625 \n",
       "Q 45.40625 18.265625 45.40625 27.984375 \n",
       "z\n",
       "M 54.390625 6.78125 \n",
       "Q 54.390625 -7.171875 48.1875 -13.984375 \n",
       "Q 42 -20.796875 29.203125 -20.796875 \n",
       "Q 24.46875 -20.796875 20.265625 -20.09375 \n",
       "Q 16.0625 -19.390625 12.109375 -17.921875 \n",
       "L 12.109375 -9.1875 \n",
       "Q 16.0625 -11.328125 19.921875 -12.34375 \n",
       "Q 23.78125 -13.375 27.78125 -13.375 \n",
       "Q 36.625 -13.375 41.015625 -8.765625 \n",
       "Q 45.40625 -4.15625 45.40625 5.171875 \n",
       "L 45.40625 9.625 \n",
       "Q 42.625 4.78125 38.28125 2.390625 \n",
       "Q 33.9375 0 27.875 0 \n",
       "Q 17.828125 0 11.671875 7.65625 \n",
       "Q 5.515625 15.328125 5.515625 27.984375 \n",
       "Q 5.515625 40.671875 11.671875 48.328125 \n",
       "Q 17.828125 56 27.875 56 \n",
       "Q 33.9375 56 38.28125 53.609375 \n",
       "Q 42.625 51.21875 45.40625 46.390625 \n",
       "L 45.40625 54.6875 \n",
       "L 54.390625 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-103\"/>\n",
       "       <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "       <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "       <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "       <path id=\"DejaVuSans-32\"/>\n",
       "       <path d=\"M 9.8125 72.90625 \n",
       "L 24.515625 72.90625 \n",
       "L 43.109375 23.296875 \n",
       "L 61.8125 72.90625 \n",
       "L 76.515625 72.90625 \n",
       "L 76.515625 0 \n",
       "L 66.890625 0 \n",
       "L 66.890625 64.015625 \n",
       "L 48.09375 14.015625 \n",
       "L 38.1875 14.015625 \n",
       "L 19.390625 64.015625 \n",
       "L 19.390625 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-77\"/>\n",
       "       <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "       <path d=\"M 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 31.109375 \n",
       "L 44.921875 54.6875 \n",
       "L 56.390625 54.6875 \n",
       "L 27.390625 29.109375 \n",
       "L 57.625 0 \n",
       "L 45.90625 0 \n",
       "L 18.109375 26.703125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-107\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(63.251875 35.557266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "       <use x=\"68.408203\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"131.787109\" xlink:href=\"#DejaVuSans-103\"/>\n",
       "       <use x=\"195.263672\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"256.787109\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "       <use x=\"284.570312\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"345.849609\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"377.636719\" xlink:href=\"#DejaVuSans-77\"/>\n",
       "       <use x=\"463.916016\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"525.439453\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"566.552734\" xlink:href=\"#DejaVuSans-107\"/>\n",
       "       <use x=\"624.416016\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"685.939453\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"text_8\">\n",
       "      <!-- Bernd Neumann -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.671875 34.8125 \n",
       "L 19.671875 8.109375 \n",
       "L 35.5 8.109375 \n",
       "Q 43.453125 8.109375 47.28125 11.40625 \n",
       "Q 51.125 14.703125 51.125 21.484375 \n",
       "Q 51.125 28.328125 47.28125 31.5625 \n",
       "Q 43.453125 34.8125 35.5 34.8125 \n",
       "z\n",
       "M 19.671875 64.796875 \n",
       "L 19.671875 42.828125 \n",
       "L 34.28125 42.828125 \n",
       "Q 41.5 42.828125 45.03125 45.53125 \n",
       "Q 48.578125 48.25 48.578125 53.8125 \n",
       "Q 48.578125 59.328125 45.03125 62.0625 \n",
       "Q 41.5 64.796875 34.28125 64.796875 \n",
       "z\n",
       "M 9.8125 72.90625 \n",
       "L 35.015625 72.90625 \n",
       "Q 46.296875 72.90625 52.390625 68.21875 \n",
       "Q 58.5 63.53125 58.5 54.890625 \n",
       "Q 58.5 48.1875 55.375 44.234375 \n",
       "Q 52.25 40.28125 46.1875 39.3125 \n",
       "Q 53.46875 37.75 57.5 32.78125 \n",
       "Q 61.53125 27.828125 61.53125 20.40625 \n",
       "Q 61.53125 10.640625 54.890625 5.3125 \n",
       "Q 48.25 0 35.984375 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-66\"/>\n",
       "       <path d=\"M 45.40625 46.390625 \n",
       "L 45.40625 75.984375 \n",
       "L 54.390625 75.984375 \n",
       "L 54.390625 0 \n",
       "L 45.40625 0 \n",
       "L 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "z\n",
       "M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-100\"/>\n",
       "       <path d=\"M 9.8125 72.90625 \n",
       "L 23.09375 72.90625 \n",
       "L 55.421875 11.921875 \n",
       "L 55.421875 72.90625 \n",
       "L 64.984375 72.90625 \n",
       "L 64.984375 0 \n",
       "L 51.703125 0 \n",
       "L 19.390625 60.984375 \n",
       "L 19.390625 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-78\"/>\n",
       "       <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "       <path d=\"M 52 44.1875 \n",
       "Q 55.375 50.25 60.0625 53.125 \n",
       "Q 64.75 56 71.09375 56 \n",
       "Q 79.640625 56 84.28125 50.015625 \n",
       "Q 88.921875 44.046875 88.921875 33.015625 \n",
       "L 88.921875 0 \n",
       "L 79.890625 0 \n",
       "L 79.890625 32.71875 \n",
       "Q 79.890625 40.578125 77.09375 44.375 \n",
       "Q 74.3125 48.1875 68.609375 48.1875 \n",
       "Q 61.625 48.1875 57.5625 43.546875 \n",
       "Q 53.515625 38.921875 53.515625 30.90625 \n",
       "L 53.515625 0 \n",
       "L 44.484375 0 \n",
       "L 44.484375 32.71875 \n",
       "Q 44.484375 40.625 41.703125 44.40625 \n",
       "Q 38.921875 48.1875 33.109375 48.1875 \n",
       "Q 26.21875 48.1875 22.15625 43.53125 \n",
       "Q 18.109375 38.875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.1875 51.21875 25.484375 53.609375 \n",
       "Q 29.78125 56 35.6875 56 \n",
       "Q 41.65625 56 45.828125 52.96875 \n",
       "Q 50 49.953125 52 44.1875 \n",
       "z\n",
       "\" id=\"DejaVuSans-109\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(52.109219 53.677266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "       <use x=\"68.603516\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"130.126953\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"171.224609\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"234.603516\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "       <use x=\"298.080078\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"329.867188\" xlink:href=\"#DejaVuSans-78\"/>\n",
       "       <use x=\"404.671875\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"466.195312\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "       <use x=\"529.574219\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "       <use x=\"626.986328\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"688.265625\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"751.644531\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"text_9\">\n",
       "      <!-- Christina Weiss -->\n",
       "      <defs>\n",
       "       <path d=\"M 64.40625 67.28125 \n",
       "L 64.40625 56.890625 \n",
       "Q 59.421875 61.53125 53.78125 63.8125 \n",
       "Q 48.140625 66.109375 41.796875 66.109375 \n",
       "Q 29.296875 66.109375 22.65625 58.46875 \n",
       "Q 16.015625 50.828125 16.015625 36.375 \n",
       "Q 16.015625 21.96875 22.65625 14.328125 \n",
       "Q 29.296875 6.6875 41.796875 6.6875 \n",
       "Q 48.140625 6.6875 53.78125 8.984375 \n",
       "Q 59.421875 11.28125 64.40625 15.921875 \n",
       "L 64.40625 5.609375 \n",
       "Q 59.234375 2.09375 53.4375 0.328125 \n",
       "Q 47.65625 -1.421875 41.21875 -1.421875 \n",
       "Q 24.65625 -1.421875 15.125 8.703125 \n",
       "Q 5.609375 18.84375 5.609375 36.375 \n",
       "Q 5.609375 53.953125 15.125 64.078125 \n",
       "Q 24.65625 74.21875 41.21875 74.21875 \n",
       "Q 47.75 74.21875 53.53125 72.484375 \n",
       "Q 59.328125 70.75 64.40625 67.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-67\"/>\n",
       "       <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "       <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "       <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "       <path d=\"M 18.3125 70.21875 \n",
       "L 18.3125 54.6875 \n",
       "L 36.8125 54.6875 \n",
       "L 36.8125 47.703125 \n",
       "L 18.3125 47.703125 \n",
       "L 18.3125 18.015625 \n",
       "Q 18.3125 11.328125 20.140625 9.421875 \n",
       "Q 21.96875 7.515625 27.59375 7.515625 \n",
       "L 36.8125 7.515625 \n",
       "L 36.8125 0 \n",
       "L 27.59375 0 \n",
       "Q 17.1875 0 13.234375 3.875 \n",
       "Q 9.28125 7.765625 9.28125 18.015625 \n",
       "L 9.28125 47.703125 \n",
       "L 2.6875 47.703125 \n",
       "L 2.6875 54.6875 \n",
       "L 9.28125 54.6875 \n",
       "L 9.28125 70.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-116\"/>\n",
       "       <path d=\"M 3.328125 72.90625 \n",
       "L 13.28125 72.90625 \n",
       "L 28.609375 11.28125 \n",
       "L 43.890625 72.90625 \n",
       "L 54.984375 72.90625 \n",
       "L 70.3125 11.28125 \n",
       "L 85.59375 72.90625 \n",
       "L 95.609375 72.90625 \n",
       "L 77.296875 0 \n",
       "L 64.890625 0 \n",
       "L 49.515625 63.28125 \n",
       "L 33.984375 0 \n",
       "L 21.578125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-87\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(57.072969 71.797266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-67\"/>\n",
       "       <use x=\"69.824219\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "       <use x=\"133.203125\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"174.316406\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"202.099609\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "       <use x=\"254.199219\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "       <use x=\"293.408203\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"321.191406\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"384.570312\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"445.849609\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"477.636719\" xlink:href=\"#DejaVuSans-87\"/>\n",
       "       <use x=\"576.435547\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"637.958984\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"665.742188\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "       <use x=\"717.841797\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"text_10\">\n",
       "      <!-- Frank-Walter Steinmeier -->\n",
       "      <defs>\n",
       "       <path d=\"M 9.8125 72.90625 \n",
       "L 51.703125 72.90625 \n",
       "L 51.703125 64.59375 \n",
       "L 19.671875 64.59375 \n",
       "L 19.671875 43.109375 \n",
       "L 48.578125 43.109375 \n",
       "L 48.578125 34.8125 \n",
       "L 19.671875 34.8125 \n",
       "L 19.671875 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-70\"/>\n",
       "       <path d=\"M 4.890625 31.390625 \n",
       "L 31.203125 31.390625 \n",
       "L 31.203125 23.390625 \n",
       "L 4.890625 23.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-45\"/>\n",
       "       <path d=\"M 53.515625 70.515625 \n",
       "L 53.515625 60.890625 \n",
       "Q 47.90625 63.578125 42.921875 64.890625 \n",
       "Q 37.9375 66.21875 33.296875 66.21875 \n",
       "Q 25.25 66.21875 20.875 63.09375 \n",
       "Q 16.5 59.96875 16.5 54.203125 \n",
       "Q 16.5 49.359375 19.40625 46.890625 \n",
       "Q 22.3125 44.4375 30.421875 42.921875 \n",
       "L 36.375 41.703125 \n",
       "Q 47.40625 39.59375 52.65625 34.296875 \n",
       "Q 57.90625 29 57.90625 20.125 \n",
       "Q 57.90625 9.515625 50.796875 4.046875 \n",
       "Q 43.703125 -1.421875 29.984375 -1.421875 \n",
       "Q 24.8125 -1.421875 18.96875 -0.25 \n",
       "Q 13.140625 0.921875 6.890625 3.21875 \n",
       "L 6.890625 13.375 \n",
       "Q 12.890625 10.015625 18.65625 8.296875 \n",
       "Q 24.421875 6.59375 29.984375 6.59375 \n",
       "Q 38.421875 6.59375 43.015625 9.90625 \n",
       "Q 47.609375 13.234375 47.609375 19.390625 \n",
       "Q 47.609375 24.75 44.3125 27.78125 \n",
       "Q 41.015625 30.8125 33.5 32.328125 \n",
       "L 27.484375 33.5 \n",
       "Q 16.453125 35.6875 11.515625 40.375 \n",
       "Q 6.59375 45.0625 6.59375 53.421875 \n",
       "Q 6.59375 63.09375 13.40625 68.65625 \n",
       "Q 20.21875 74.21875 32.171875 74.21875 \n",
       "Q 37.3125 74.21875 42.625 73.28125 \n",
       "Q 47.953125 72.359375 53.515625 70.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-83\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 89.917266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-70\"/>\n",
       "       <use x=\"57.410156\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"98.523438\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"159.802734\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"223.181641\" xlink:href=\"#DejaVuSans-107\"/>\n",
       "       <use x=\"281.091797\" xlink:href=\"#DejaVuSans-45\"/>\n",
       "       <use x=\"317.113281\" xlink:href=\"#DejaVuSans-87\"/>\n",
       "       <use x=\"415.896484\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"477.175781\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "       <use x=\"504.958984\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "       <use x=\"544.167969\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"605.691406\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"646.804688\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"678.591797\" xlink:href=\"#DejaVuSans-83\"/>\n",
       "       <use x=\"742.068359\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "       <use x=\"781.277344\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"842.800781\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"870.583984\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"933.962891\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "       <use x=\"1031.375\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"1092.898438\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"1120.681641\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"1182.205078\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"text_11\">\n",
       "      <!-- Gerhard Schröder -->\n",
       "      <defs>\n",
       "       <path d=\"M 59.515625 10.40625 \n",
       "L 59.515625 29.984375 \n",
       "L 43.40625 29.984375 \n",
       "L 43.40625 38.09375 \n",
       "L 69.28125 38.09375 \n",
       "L 69.28125 6.78125 \n",
       "Q 63.578125 2.734375 56.6875 0.65625 \n",
       "Q 49.8125 -1.421875 42 -1.421875 \n",
       "Q 24.90625 -1.421875 15.25 8.5625 \n",
       "Q 5.609375 18.5625 5.609375 36.375 \n",
       "Q 5.609375 54.25 15.25 64.234375 \n",
       "Q 24.90625 74.21875 42 74.21875 \n",
       "Q 49.125 74.21875 55.546875 72.453125 \n",
       "Q 61.96875 70.703125 67.390625 67.28125 \n",
       "L 67.390625 56.78125 \n",
       "Q 61.921875 61.421875 55.765625 63.765625 \n",
       "Q 49.609375 66.109375 42.828125 66.109375 \n",
       "Q 29.4375 66.109375 22.71875 58.640625 \n",
       "Q 16.015625 51.171875 16.015625 36.375 \n",
       "Q 16.015625 21.625 22.71875 14.15625 \n",
       "Q 29.4375 6.6875 42.828125 6.6875 \n",
       "Q 48.046875 6.6875 52.140625 7.59375 \n",
       "Q 56.25 8.5 59.515625 10.40625 \n",
       "z\n",
       "\" id=\"DejaVuSans-71\"/>\n",
       "       <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "       <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "M 35.21875 75.78125 \n",
       "L 45.125 75.78125 \n",
       "L 45.125 65.921875 \n",
       "L 35.21875 65.921875 \n",
       "z\n",
       "M 16.125 75.78125 \n",
       "L 26.03125 75.78125 \n",
       "L 26.03125 65.921875 \n",
       "L 16.125 65.921875 \n",
       "z\n",
       "\" id=\"DejaVuSans-246\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(43.713125 108.037266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "       <use x=\"77.490234\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"139.013672\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"180.111328\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "       <use x=\"243.490234\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"304.769531\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"345.867188\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "       <use x=\"409.34375\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"441.130859\" xlink:href=\"#DejaVuSans-83\"/>\n",
       "       <use x=\"504.607422\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "       <use x=\"559.587891\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "       <use x=\"622.966797\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"664.048828\" xlink:href=\"#DejaVuSans-246\"/>\n",
       "       <use x=\"725.230469\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "       <use x=\"788.707031\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"850.230469\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"text_12\">\n",
       "      <!-- Hans Martin Bury -->\n",
       "      <defs>\n",
       "       <path d=\"M 9.8125 72.90625 \n",
       "L 19.671875 72.90625 \n",
       "L 19.671875 43.015625 \n",
       "L 55.515625 43.015625 \n",
       "L 55.515625 72.90625 \n",
       "L 65.375 72.90625 \n",
       "L 65.375 0 \n",
       "L 55.515625 0 \n",
       "L 55.515625 34.71875 \n",
       "L 19.671875 34.71875 \n",
       "L 19.671875 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-72\"/>\n",
       "       <path d=\"M 32.171875 -5.078125 \n",
       "Q 28.375 -14.84375 24.75 -17.8125 \n",
       "Q 21.140625 -20.796875 15.09375 -20.796875 \n",
       "L 7.90625 -20.796875 \n",
       "L 7.90625 -13.28125 \n",
       "L 13.1875 -13.28125 \n",
       "Q 16.890625 -13.28125 18.9375 -11.515625 \n",
       "Q 21 -9.765625 23.484375 -3.21875 \n",
       "L 25.09375 0.875 \n",
       "L 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 11.921875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-121\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(46.411562 126.157266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-72\"/>\n",
       "       <use x=\"75.195312\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"136.474609\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"199.853516\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "       <use x=\"251.953125\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"283.740234\" xlink:href=\"#DejaVuSans-77\"/>\n",
       "       <use x=\"370.019531\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"431.298828\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"472.412109\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "       <use x=\"511.621094\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"539.404297\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"602.783203\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"634.570312\" xlink:href=\"#DejaVuSans-66\"/>\n",
       "       <use x=\"703.173828\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "       <use x=\"766.552734\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"807.666016\" xlink:href=\"#DejaVuSans-121\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"text_13\">\n",
       "      <!-- Joschka Fischer -->\n",
       "      <defs>\n",
       "       <path d=\"M 9.8125 72.90625 \n",
       "L 19.671875 72.90625 \n",
       "L 19.671875 5.078125 \n",
       "Q 19.671875 -8.109375 14.671875 -14.0625 \n",
       "Q 9.671875 -20.015625 -1.421875 -20.015625 \n",
       "L -5.171875 -20.015625 \n",
       "L -5.171875 -11.71875 \n",
       "L -2.09375 -11.71875 \n",
       "Q 4.4375 -11.71875 7.125 -8.046875 \n",
       "Q 9.8125 -4.390625 9.8125 5.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-74\"/>\n",
       "       <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(57.021406 144.277266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "       <use x=\"29.492188\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "       <use x=\"90.673828\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "       <use x=\"142.773438\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "       <use x=\"197.753906\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "       <use x=\"261.132812\" xlink:href=\"#DejaVuSans-107\"/>\n",
       "       <use x=\"319.027344\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"380.306641\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"412.09375\" xlink:href=\"#DejaVuSans-70\"/>\n",
       "       <use x=\"469.503906\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"497.287109\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "       <use x=\"549.386719\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "       <use x=\"604.367188\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "       <use x=\"667.746094\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"729.269531\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"text_14\">\n",
       "      <!-- Julian Nida-Rümelin -->\n",
       "      <defs>\n",
       "       <path d=\"M 44.390625 34.1875 \n",
       "Q 47.5625 33.109375 50.5625 29.59375 \n",
       "Q 53.5625 26.078125 56.59375 19.921875 \n",
       "L 66.609375 0 \n",
       "L 56 0 \n",
       "L 46.6875 18.703125 \n",
       "Q 43.0625 26.03125 39.671875 28.421875 \n",
       "Q 36.28125 30.8125 30.421875 30.8125 \n",
       "L 19.671875 30.8125 \n",
       "L 19.671875 0 \n",
       "L 9.8125 0 \n",
       "L 9.8125 72.90625 \n",
       "L 32.078125 72.90625 \n",
       "Q 44.578125 72.90625 50.734375 67.671875 \n",
       "Q 56.890625 62.453125 56.890625 51.90625 \n",
       "Q 56.890625 45.015625 53.6875 40.46875 \n",
       "Q 50.484375 35.9375 44.390625 34.1875 \n",
       "z\n",
       "M 19.671875 64.796875 \n",
       "L 19.671875 38.921875 \n",
       "L 32.078125 38.921875 \n",
       "Q 39.203125 38.921875 42.84375 42.21875 \n",
       "Q 46.484375 45.515625 46.484375 51.90625 \n",
       "Q 46.484375 58.296875 42.84375 61.546875 \n",
       "Q 39.203125 64.796875 32.078125 64.796875 \n",
       "z\n",
       "\" id=\"DejaVuSans-82\"/>\n",
       "       <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "M 35.59375 75.78125 \n",
       "L 45.5 75.78125 \n",
       "L 45.5 65.921875 \n",
       "L 35.59375 65.921875 \n",
       "z\n",
       "M 16.5 75.78125 \n",
       "L 26.40625 75.78125 \n",
       "L 26.40625 65.921875 \n",
       "L 16.5 65.921875 \n",
       "z\n",
       "\" id=\"DejaVuSans-252\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(34.074375 162.397266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "       <use x=\"29.492188\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "       <use x=\"92.871094\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "       <use x=\"120.654297\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"148.4375\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"209.716797\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"273.095703\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"304.882812\" xlink:href=\"#DejaVuSans-78\"/>\n",
       "       <use x=\"379.6875\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"407.470703\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "       <use x=\"470.947266\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"532.226562\" xlink:href=\"#DejaVuSans-45\"/>\n",
       "       <use x=\"568.310547\" xlink:href=\"#DejaVuSans-82\"/>\n",
       "       <use x=\"637.730469\" xlink:href=\"#DejaVuSans-252\"/>\n",
       "       <use x=\"701.109375\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "       <use x=\"798.521484\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"860.044922\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "       <use x=\"887.828125\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"915.611328\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"text_15\">\n",
       "      <!-- Michael Naumann -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(42.795312 180.517266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-77\"/>\n",
       "       <use x=\"86.279297\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"114.0625\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "       <use x=\"169.042969\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "       <use x=\"232.421875\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"293.701172\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"355.224609\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "       <use x=\"383.007812\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"414.794922\" xlink:href=\"#DejaVuSans-78\"/>\n",
       "       <use x=\"489.599609\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"550.878906\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "       <use x=\"614.257812\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "       <use x=\"711.669922\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"772.949219\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"836.328125\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"text_16\">\n",
       "      <!-- Monika Grütters -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(53.247031 198.637266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-77\"/>\n",
       "       <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "       <use x=\"147.460938\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"210.839844\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"238.623047\" xlink:href=\"#DejaVuSans-107\"/>\n",
       "       <use x=\"296.517578\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"357.796875\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"389.583984\" xlink:href=\"#DejaVuSans-71\"/>\n",
       "       <use x=\"467.074219\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"508.1875\" xlink:href=\"#DejaVuSans-252\"/>\n",
       "       <use x=\"571.566406\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "       <use x=\"610.775391\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "       <use x=\"649.984375\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"711.507812\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"752.621094\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"text_17\">\n",
       "      <!-- Rolf Schwanitz -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.109375 75.984375 \n",
       "L 37.109375 68.5 \n",
       "L 28.515625 68.5 \n",
       "Q 23.6875 68.5 21.796875 66.546875 \n",
       "Q 19.921875 64.59375 19.921875 59.515625 \n",
       "L 19.921875 54.6875 \n",
       "L 34.71875 54.6875 \n",
       "L 34.71875 47.703125 \n",
       "L 19.921875 47.703125 \n",
       "L 19.921875 0 \n",
       "L 10.890625 0 \n",
       "L 10.890625 47.703125 \n",
       "L 2.296875 47.703125 \n",
       "L 2.296875 54.6875 \n",
       "L 10.890625 54.6875 \n",
       "L 10.890625 58.5 \n",
       "Q 10.890625 67.625 15.140625 71.796875 \n",
       "Q 19.390625 75.984375 28.609375 75.984375 \n",
       "z\n",
       "\" id=\"DejaVuSans-102\"/>\n",
       "       <path d=\"M 4.203125 54.6875 \n",
       "L 13.1875 54.6875 \n",
       "L 24.421875 12.015625 \n",
       "L 35.59375 54.6875 \n",
       "L 46.1875 54.6875 \n",
       "L 57.421875 12.015625 \n",
       "L 68.609375 54.6875 \n",
       "L 77.59375 54.6875 \n",
       "L 63.28125 0 \n",
       "L 52.6875 0 \n",
       "L 40.921875 44.828125 \n",
       "L 29.109375 0 \n",
       "L 18.5 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-119\"/>\n",
       "       <path d=\"M 5.515625 54.6875 \n",
       "L 48.1875 54.6875 \n",
       "L 48.1875 46.484375 \n",
       "L 14.40625 7.171875 \n",
       "L 48.1875 7.171875 \n",
       "L 48.1875 0 \n",
       "L 4.296875 0 \n",
       "L 4.296875 8.203125 \n",
       "L 38.09375 47.515625 \n",
       "L 5.515625 47.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-122\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(61.118906 216.757266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-82\"/>\n",
       "       <use x=\"69.419922\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "       <use x=\"130.601562\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "       <use x=\"158.384766\" xlink:href=\"#DejaVuSans-102\"/>\n",
       "       <use x=\"193.589844\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"225.376953\" xlink:href=\"#DejaVuSans-83\"/>\n",
       "       <use x=\"288.853516\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "       <use x=\"343.833984\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "       <use x=\"407.212891\" xlink:href=\"#DejaVuSans-119\"/>\n",
       "       <use x=\"489\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"550.279297\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "       <use x=\"613.658203\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"641.441406\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "       <use x=\"680.650391\" xlink:href=\"#DejaVuSans-122\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"text_18\">\n",
       "      <!-- Thomas de Maizière -->\n",
       "      <defs>\n",
       "       <path d=\"M -0.296875 72.90625 \n",
       "L 61.375 72.90625 \n",
       "L 61.375 64.59375 \n",
       "L 35.5 64.59375 \n",
       "L 35.5 0 \n",
       "L 25.59375 0 \n",
       "L 25.59375 64.59375 \n",
       "L -0.296875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-84\"/>\n",
       "       <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "M 24.671875 79.984375 \n",
       "L 38.4375 61.71875 \n",
       "L 30.96875 61.71875 \n",
       "L 15.046875 79.984375 \n",
       "z\n",
       "\" id=\"DejaVuSans-232\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(31.240156 234.877266)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#DejaVuSans-84\"/>\n",
       "       <use x=\"61.083984\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "       <use x=\"124.462891\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "       <use x=\"185.644531\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "       <use x=\"283.056641\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"344.335938\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "       <use x=\"396.435547\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"428.222656\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "       <use x=\"491.699219\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "       <use x=\"553.222656\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"585.009766\" xlink:href=\"#DejaVuSans-77\"/>\n",
       "       <use x=\"671.289062\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "       <use x=\"732.568359\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"760.351562\" xlink:href=\"#DejaVuSans-122\"/>\n",
       "       <use x=\"812.841797\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "       <use x=\"840.625\" xlink:href=\"#DejaVuSans-232\"/>\n",
       "       <use x=\"902.148438\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "       <use x=\"943.230469\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 24.130125 \n",
       "L 470.119799 24.130125 \n",
       "L 470.119799 38.626125 \n",
       "L 151.262656 38.626125 \n",
       "z\n",
       "\" style=\"fill:#ea96a3;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 42.250125 \n",
       "L 470.119799 42.250125 \n",
       "L 470.119799 56.746125 \n",
       "L 151.262656 56.746125 \n",
       "z\n",
       "\" style=\"fill:#e19153;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 60.370125 \n",
       "L 470.119799 60.370125 \n",
       "L 470.119799 74.866125 \n",
       "L 151.262656 74.866125 \n",
       "z\n",
       "\" style=\"fill:#b89c49;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 78.490125 \n",
       "L 195.902656 78.490125 \n",
       "L 195.902656 92.986125 \n",
       "L 151.262656 92.986125 \n",
       "z\n",
       "\" style=\"fill:#98a246;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 96.610125 \n",
       "L 470.119799 96.610125 \n",
       "L 470.119799 111.106125 \n",
       "L 151.262656 111.106125 \n",
       "z\n",
       "\" style=\"fill:#60ae47;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 114.730125 \n",
       "L 419.102656 114.730125 \n",
       "L 419.102656 129.226125 \n",
       "L 151.262656 129.226125 \n",
       "z\n",
       "\" style=\"fill:#4aae8a;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 132.850125 \n",
       "L 348.954085 132.850125 \n",
       "L 348.954085 147.346125 \n",
       "L 151.262656 147.346125 \n",
       "z\n",
       "\" style=\"fill:#4baba4;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 150.970125 \n",
       "L 457.365513 150.970125 \n",
       "L 457.365513 165.466125 \n",
       "L 151.262656 165.466125 \n",
       "z\n",
       "\" style=\"fill:#4fabbc;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 169.090125 \n",
       "L 470.119799 169.090125 \n",
       "L 470.119799 183.586125 \n",
       "L 151.262656 183.586125 \n",
       "z\n",
       "\" style=\"fill:#6daee2;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 187.210125 \n",
       "L 470.119799 187.210125 \n",
       "L 470.119799 201.706125 \n",
       "L 151.262656 201.706125 \n",
       "z\n",
       "\" style=\"fill:#b6a8eb;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 205.330125 \n",
       "L 304.314085 205.330125 \n",
       "L 304.314085 219.826125 \n",
       "L 151.262656 219.826125 \n",
       "z\n",
       "\" style=\"fill:#df8fe7;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path clip-path=\"url(#p9cd2f3f24a)\" d=\"M 151.262656 223.450125 \n",
       "L 425.479799 223.450125 \n",
       "L 425.479799 237.946125 \n",
       "L 151.262656 237.946125 \n",
       "z\n",
       "\" style=\"fill:#e890c6;stroke:#ffffff;stroke-linejoin:miter;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 151.262656 239.758125 \n",
       "L 151.262656 22.318125 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 486.062656 239.758125 \n",
       "L 486.062656 22.318125 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 151.262656 239.758125 \n",
       "L 486.062656 239.758125 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 151.262656 22.318125 \n",
       "L 486.062656 22.318125 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_19\">\n",
       "    <!-- Anzahl der Reden -->\n",
       "    <g style=\"fill:#262626;\" transform=\"translate(265.518594 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "     <use x=\"68.408203\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"131.787109\" xlink:href=\"#DejaVuSans-122\"/>\n",
       "     <use x=\"184.277344\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"245.556641\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "     <use x=\"308.935547\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"336.71875\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"368.505859\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "     <use x=\"431.982422\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"493.505859\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"534.619141\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"566.40625\" xlink:href=\"#DejaVuSans-82\"/>\n",
       "     <use x=\"635.826172\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"697.349609\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "     <use x=\"760.826172\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"822.349609\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p9cd2f3f24a\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"151.262656\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg' # schönere Grafiken\n",
    "sns.set()\n",
    "\n",
    "_ = sns.countplot(y=\"person\", data=df) \\\n",
    "       .set(title=\"Anzahl der Reden\", xlabel=\"\", ylabel=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "## Analyse der Reden &mdash; von Daten zu Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "Die Reden liegen uns nun als Python-Strings vor. Zur Anwendung von machine learning oder deep learning müssen wir sie nun mit Hilfe von NLP weiterverarbeiten. Dafür gibt es zahlreiche Python-Bibliotheken wie\n",
    "\n",
    "- [NLTK](https://www.nltk.org/) (\"the natural language toolkit\"),\n",
    "- [spaCy](https://spacy.io/) (\"industrial strength natural language processing\"),\n",
    "- [gensim](https://radimrehurek.com/gensim/) (\"topic modelling for humans\")\n",
    "\n",
    "und viele andere. Wir verwenden in diesem Tutorial spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Automatische Textanalyse mit spaCy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als Erstes benötigt spaCy ein \"Modell\" der Sprache, mit der es arbeiten soll. Dieses Modell enthält statistische Informationen über die Sprache und wird für Deutsch von spaCy bereitgestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Rufen wir  `nlp` mit einem Python-String auf, so wendet spaCy seine [Textanalyse-Pipeline](https://spacy.io/usage/processing-pipelines) darauf an und\n",
    "\n",
    "1. zerlegt den Text in Token ([Tokenisierung](https://de.wikipedia.org/wiki/Tokenisierung)),\n",
    "2. bestimmt die Wortstämme beziehungsweise Grundformen der Token ([Lemmatisierung](https://de.wikipedia.org/wiki/Lemma_(Lexikographie)#Lemmatisierung)),\n",
    "3. bestimmt die Wortart ([part-of-speech tagging](https://de.wikipedia.org/wiki/Part-of-speech-Tagging)),\n",
    "4. analysiert die grammatikalische Struktur der Text und\n",
    "5. extrahiert Begriffe ([named entity recognition](https://de.wikipedia.org/wiki/Part-of-speech-Tagging)).\n",
    "\n",
    "Schauen wir uns die Ergebnisse der Schritte 1-3 für einen Beispielsatz an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Grundform</th>\n",
       "      <th>Wortart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Peter</td>\n",
       "      <td>Peter</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fährt</td>\n",
       "      <td>fahren</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Berlin</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>am</td>\n",
       "      <td>am</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>schnellsten</td>\n",
       "      <td>schnell</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>auf</td>\n",
       "      <td>auf</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dem</td>\n",
       "      <td>der</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kaputten</td>\n",
       "      <td>kaputt</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kunterbunten</td>\n",
       "      <td>kunterbunten</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fahrrad</td>\n",
       "      <td>Fahrrad</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Token     Grundform Wortart\n",
       "0          Peter         Peter   PROPN\n",
       "1          fährt        fahren    VERB\n",
       "2             in            in     ADP\n",
       "3         Berlin        Berlin   PROPN\n",
       "4             am            am    PART\n",
       "5    schnellsten       schnell     ADJ\n",
       "6            auf           auf     ADP\n",
       "7            dem           der     DET\n",
       "8       kaputten        kaputt     ADJ\n",
       "9   kunterbunten  kunterbunten     ADJ\n",
       "10       Fahrrad       Fahrrad    NOUN\n",
       "11             .             .   PUNCT"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = nlp(\"Peter fährt in Berlin am schnellsten auf dem kaputten kunterbunten Fahrrad.\")\n",
    "pd.DataFrame({\"Token\": [word.text for word in document],\n",
    "              \"Grundform\": [word.lemma_ for word in document],\n",
    "              \"Wortart\": [word.pos_ for word in document]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir sehen, dass Schritt 2 nicht immer korrekte Ergebnisse liefert. Grund dafür ist zum Beispiel, dass das Modell nicht alle Wörter kennt. Außerdem verwendet spaCy selbst nicht eine Sammlung von Regeln, sondern neuronale Netze mit entsprechend unscharfen Ergebnissen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Anwendung auf die Reden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun wenden wir spaCy auf unsere Reden an und\n",
    "\n",
    "- extrahieren die jeweiligen Folgen von Token, Grundformen und Begriffen,\n",
    "- schalten dabei nicht benutzte Pipeline-Schritte wie die grammatikalische Analyse ab,\n",
    "- tragen die Ergebnisse in neue Spalten in unserem DataFrame `df` ein.\n",
    "\n",
    "Weil das eine Weile dauert, speichern wir die Ergebnisse mittels [pickle](https://docs.python.org/3/library/pickle.html) ab und lesen sie das nächste Mal einfach wieder ein. Zeit für einen Kaffee?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ANALYSIS_FILE = \"speeches.pickle\"\n",
    "ANALYSIS_PATH = os.path.join(DATA_PATH, ANALYSIS_FILE)\n",
    "\n",
    "def analyze(speech):\n",
    "    with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "        document = nlp(speech)\n",
    "        token = [w.text for w in document]\n",
    "        lemma = [w.lemma_ for w in document]\n",
    "        entities = [e.text for e in document.ents]\n",
    "        return (token, lemma, entities)\n",
    "    \n",
    "if not os.path.isfile(ANALYSIS_PATH):\n",
    "    df[\"analysis\"] = df.speech.map(analyze)\n",
    "    df[\"tokens\"] = df.analysis.apply(lambda x: x[0])\n",
    "    df[\"lemmata\"] = df.analysis.apply(lambda x: x[1])\n",
    "    df[\"entities\"] = df.analysis.apply(lambda x: x[2])\n",
    "    df.to_pickle(ANALYSIS_PATH)\n",
    "else:\n",
    "    df = pd.read_pickle(ANALYSIS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Unser DataFrame sieht nun wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "###  Von Token zu Statistiken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die mit spaCy analysierten Reden verarbeiten wir jetzt weiter, indem wir\n",
    "\n",
    "1. das _Gesamt-Vokabular_ bestimmen, also die Menge aller Token, die in den Reden verwendet werden;\n",
    "2. das Gesamt-Vokabular durchnummerieren und ein Python-Dictionary `word2index` anlegen, das jedem Token seine Nummer zuordnet;\n",
    "3. für jede Rede  ermitteln, _welche_  Token darin auftauchen (die Menge ist das _bag of words_ der Rede) und _wie oft_ jedes Token darin auftaucht.\n",
    "\n",
    "Dasselbe machen wir auch für die _Grundformen_ der Token und die _extrahierten Begriffe_. Weitere interessante statistische Informationen wären etwa die _relative Häufigkeit_ oder das [tf-idf-Maß](https://de.wikipedia.org/wiki/Tf-idf-Ma%C3%9F) der Token.\n",
    "\n",
    "Dafür gibt es natürlich bereits fertige [Routinen](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text) in Bibliotheken wie [scikit-learn](https://sklearn.org). Aber erstens können wir diese Schritte einfach selbst bewältigen, und zweitens lernt man dabei mehr!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def bag_of_words(speeches):\n",
    "    word_sets = [set(speech) for speech in speeches]\n",
    "    all_words = set.union(*word_sets)\n",
    "    word2index = {word: index for (index, word) in enumerate(all_words)}\n",
    "    indexed_speeches = speeches.apply(lambda speech: [word2index[word]\n",
    "                                                      for word in speech])\n",
    "    return (indexed_speeches, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Ein Beispiel macht klarer, was passiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "example = pd.DataFrame({\"Rede\": [\\\n",
    "    [\"Hallo\", \"Welt\", \"Du\", \"bist\", \"so\", \"schön\"], \\\n",
    "    [\"Hallo\", \"bist\", \"Du\", \"wach\", \"Du\"], \\\n",
    "    [\"die\", \"Welt\", \"ist\", \"groß\", \"ist\", \"die\", \"Welt\"]]}, index=[\"\",\"\",\"\"])\n",
    "\n",
    "example[\"Bag of Words\"], example_index =  bag_of_words(example.Rede)\n",
    "pd.DataFrame(example_index, index=[\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als zweite Variante zählen wir für jedes Token einer Rede, wie oft es in der Rede auftritt &mdash; daher die Bezeichnung `count`. Dafür ist die Klasse [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) hilfreich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_words(speeches, word2index):\n",
    "    def count_speech(speech):\n",
    "        return {word2index[word]: count\n",
    "                for (word, count) in Counter(speech).items()}\n",
    "    return speeches.apply(count_speech)\n",
    "\n",
    "example[\"Counts\"] = count_words(example[\"Rede\"], example_index)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Für eine einheitliche Weiterverarbeitung wandeln wir die bags-of-words ebenfalls in Dictionarys um, die jedem Token einfach eine 1 zuordnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def bags_of_words_to_dicts(bags_of_words):\n",
    "    return bags_of_words.apply(lambda bow: {word: 1 for word in bow})\n",
    "\n",
    "example[\"Bag of Words as dict\"] = bags_of_words_to_dicts(example[\"Bag of Words\"])\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir wenden all das nun auf unsere Reden an und verwenden dabei nicht nur die Token, sondern auch deren Grundformen und die extrahierten Begriffe. Die Ergebnisse tragen wir wieder in unseren DataFrame `df` in neue Spalten ein. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "PARTS = [\"tokens\", \"lemmata\", \"entities\"]\n",
    "INDEX = dict()\n",
    "\n",
    "for part in PARTS:\n",
    "    (bow, INDEX[part]) = bag_of_words(df[part])\n",
    "    df[part + \"_bow\"] = bags_of_words_to_dicts(bow)\n",
    "    df[part + \"_count\"] = count_words(df[part], INDEX[part])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Exemplarisch betrachten wir einige der extrahierten Daten der ersten Rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "## Klassifikation der Reden mit machine learning und scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "Wir trainieren nun ein Modell darauf, zu einer gegebenen Rede den jeweiligen Redner zu bestimmen! Dazu verwenden wir\n",
    "\n",
    "- als \"Fingerabdruck\" jeder Rede die extrahierten statistischen Informationen und\n",
    "- als Modell zuerst ein klassisches _machine learning_-Verfahren, einen [Bayes-Klassifikator](https://de.wikipedia.org/wiki/Bayes-Klassifikator),\n",
    "- die Standard-machine learning-Bibliothek [scikit-learn](https://sklearn.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Vorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Obwohl wir die Daten bereits aufbereitet haben, sind für das Training und das Testen des Modells noch ein paar Vorbereitungen erforderlich.\n",
    "\n",
    "Um zu sehen, welchen Einfluss die Datenmenge und -auswahl auf die Genauigkeit der Klassifikation hat, wählen wir zunächst mit `sample_speeches` pro Redner eine feste Anzahl `num_samples`von Reden zufällig aus dem Datensatz `data` aus. Sind nicht genügend Reden vorhanden, wird der Redner herausgefiltert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def sample_speeches(data, num_samples):\n",
    "    # lokale Hilfsfunktion, führt Sampling und Entfernung von Personen aus\n",
    "    def sample(group):\n",
    "        if len(group) >= num_samples:\n",
    "            return group.sample(num_samples)\n",
    "        else:\n",
    "            return group.sample(0)\n",
    "    return data.groupby(\"person\", group_keys=False) \\\n",
    "               .apply(sample) \\\n",
    "               .reset_index(drop=True)\n",
    "\n",
    "# beispielhaftes Sampling\n",
    "sampled_df = sample_speeches(df, 20)\n",
    "sampled_df[\"person\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als nächstes zerlegen wir mit `split` die aufbereiteten Daten `data` &mdash; die Fingerabdrücke der Reden &mdash; und die zugehörigen `label` &mdash; die Liste der jeweiligen Redner &mdash; im Verhältnis `ratio` zu 1-`ratio` in ein Trainings-Daten und Test-Daten. Dabei erfolgt die Auswahl zufällig und die Daten werden permutiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def split(data, labels, ratio):\n",
    "    size = data.shape[0]\n",
    "    split_index = int(ratio * size)\n",
    "    indices = np.random.permutation(size)\n",
    "    train_indices, test_indices = (indices[:split_index], indices[split_index:])\n",
    "    return (data[train_indices], labels[train_indices],\n",
    "            data[test_indices], labels[test_indices])\n",
    "\n",
    "# beispielhaftes Zerlegen, sodass zwei Trainingsdatenpunkte entstehen\n",
    "train_word, train_label, test_word, test_label = split(pd.Series([\"Das\", \"ist\", \"ein\", \"Test\"]), \n",
    "                                                       pd.Series([1, 2, 3, 4]), 0.6)\n",
    "train_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Für jede Rede haben wir bisher die statistischen Informationen in Python-Dictionarys abgelegt. Der Bayes-Klassifikator benötigt diese Daten aber in Form eines zweidimensionalen Arrays, bei dem jede Zeile einer Rede entspricht, jede Spalte einem Token (bzw. Grundform oder Begriff) des Gesamtvokabulars. Da das Gesamtvokabular sehr groß ist, in jeder Rede aber davon nur ein sehr kleiner Bruchteil verwendet wird, würde dieses Array hauptsächlich Nullen enthalten. Das Modul\n",
    "[scipy.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html) bietet für solche [dünn besetzten](https://de.wikipedia.org/wiki/D%C3%BCnnbesetzte_Matrix) Arrays speziell optimierte Datenstrukturen.\n",
    "Mit `dict_to_sparse` wandeln wir nun eine Liste der Python-Dictionarys in eine dünn besetzte Matrix um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "import itertools\n",
    "\n",
    "def dict_to_sparse(list_of_dicts, vocab_size):\n",
    "    index_and_data = [(index, dictionary[index])  for dictionary in list_of_dicts \n",
    "                          for index in dictionary]\n",
    "    index, data = zip(*index_and_data)\n",
    "    lens = [len(dictionary) for dictionary in list_of_dicts]\n",
    "    row_pointers = list(itertools.accumulate([0] + lens))\n",
    "    return sparse.csr_matrix((data, index, row_pointers), \n",
    "                             shape=(len(list_of_dicts), vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dict_to_sparse(example[\"Bag of Words as dict\"], len(example_index)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Training und Test des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Training und Test des Bayes-Klassifikators kapseln wir in eine separate Routine `train_and_test`. Neben den Daten und Labels erwartet diese als Parameter auch das zu wählende Verhältnis zwischen Trainingsdaten und Testdaten.\n",
    "Die Methode `fit` führt das eigentliche Training durch und die Methode `predict` den Test. Die Genauigkeit des Modells messen wir wie folgt:\n",
    "\n",
    "- `accuracy` gibt an, welcher Anteil der Vorhersagen korrekt war, ist also ein einfaches Maß für die Genauigkeit;\n",
    "- `confusion` ist eine Matrix, deren Zeilen und Spalten mit den vorhandenen Rednern durchnummeriert sind und deren i-te Zeile und j-te Spalte angibt, wieviel Reden des i-ten Redners von dem Modell für eine Rede des j-ten Redners gehalten wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train_and_test(data, labels, split_ratio):\n",
    "    train_data, train_labels, test_data, test_labels = split(data, labels,\n",
    "                                                             split_ratio)\n",
    "    classifier = MultinomialNB(alpha=0.05)\n",
    "    classifier.fit(train_data, train_labels)\n",
    "    predictions = classifier.predict(test_data)\n",
    "    accuracy = np.mean(predictions == test_labels)\n",
    "    confusion = confusion_matrix(test_labels, predictions)\n",
    "    return (accuracy, confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Das Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun kommen wir zum eigentlichen Experiment! Wir trainieren und testen nacheinander und unabhängig voneinander einen Bayes-Klassifikator mit den extrahierten statistischen Informationen und machen für jede Kombination von\n",
    "Features und Statistiken mehrere Durchläufe. Anschließend visualisieren wir die Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "FEATURES = [\"tokens\", \"lemmata\", \"entities\"]\n",
    "STATS = [\"bow\", \"count\"]\n",
    "REPETITIONS = 60\n",
    "SPLIT_RATIO = 0.7\n",
    "NUM_SAMPLES = 50\n",
    "\n",
    "def bayes_experiment(features=FEATURES, stats=STATS, repetitions=REPETITIONS,\n",
    "                     num_samples=NUM_SAMPLES, split_ratio=SPLIT_RATIO):\n",
    "    sampled_df = sample_speeches(df, num_samples)\n",
    "    labels = sampled_df[\"person\"].astype(\"category\").cat.codes\n",
    "    print(\"Anzahl der Redner: \", np.max(labels)+1)\n",
    "    results = []\n",
    "    for feature in features:\n",
    "        for stat in stats:\n",
    "            data = dict_to_sparse(sampled_df[feature + \"_\" + stat],\n",
    "                                  len(INDEX[feature]))\n",
    "            print(\"Klassifizierung anhand Statistik '%s' für Feature '%s'\" \n",
    "                  % (stat, feature))\n",
    "            for i in range(0, repetitions):\n",
    "                (accuracy, confusion) = train_and_test(data, labels, split_ratio)\n",
    "                results.append({\"feature\": feature, \n",
    "                                \"stat\": stat, \n",
    "                                \"repetition\": i,\n",
    "                                \"accuracy\": accuracy, \n",
    "                                \"confusion_matrix\": confusion})\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir führen das Experiment zuerst mit 25 Reden pro Person durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "results_few_samples = bayes_experiment(num_samples = 25)\n",
    "results_few_samples.groupby([\"feature\", \"stat\"])[\"accuracy\"].mean().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die erzielte Genauigkeit der Vorhersagen ist recht gering &mdash; durch zufälliges Raten würden wir bei 10 Rednern im Schnitt eine Genauigkeit von 0.1 beziehungsweise 10% erzielen.\n",
    "\n",
    "Folgender Plot zeigt, dass die Genauigkeit außerdem stark von der zufälligen Auswahl der Trainings- und Testdaten abhängt, aber kaum davon, welche Features oder Statistik wir verwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(data, title, kind=\"swarm\"):\n",
    "    sns.catplot(data=data, col=\"feature\", x=\"stat\", y=\"accuracy\", kind=kind) \\\n",
    "        .set(xlabel=\"verwendete Satistik\", ylabel=\"Genauigkeit\")\n",
    "    plt.subplots_adjust(top=0.88)\n",
    "    plt.suptitle(title)\n",
    "    \n",
    "plot_accuracies(results_few_samples, \"Bei wenig Daten schwankt die Genauigkeit der Klassifikation stark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als nächstes plotten wir heatmaps für die  [Konfusionsmatrizen](https://en.wikipedia.org/wiki/Confusion_matrix). Wie bereits erklärt geben diese an, wieviel Reden richtig (Diagonaleinträge) beziehungsweise falsch (die anderen Einträge) zugeordnet wurden. Wir gruppieren diese Matrizen nach Feature und Statistik und summieren über die Durchläufe auf.\n",
    "\n",
    "So zeigt sich, dass zum Beispiel für die Features Lemmata und Tokens (die beiden rechten Spalten) ein bis zwei Redner bevorzugt vorausgesagt wurden (\"helle\" Spalten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(results):\n",
    "    def plot_heatmap(data, **kwargs):\n",
    "        sns.heatmap(data[\"confusion_matrix\"].iloc[0])\n",
    "    confusion_matrix = results.groupby([\"feature\", \"stat\"])[\"confusion_matrix\"] \\\n",
    "                              .apply(lambda cms: np.sum(cms, axis=0)) \\\n",
    "                              .reset_index()   \n",
    "    g = sns.FacetGrid(confusion_matrix, col=\"feature\", row=\"stat\")\n",
    "    g.map_dataframe(plot_heatmap)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.suptitle(\"Summierte 'Confusion'-Matrizen der Klassifikation\")\n",
    "\n",
    "plot_confusion_matrices(results_few_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun führen wir dasselbe Experiment noch einmal mit 50 Reden pro Person durch und schauen, wie sich die Vorhersagegenauigkeit verbessert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "results_many_samples = bayes_experiment(num_samples=50)\n",
    "results_many_samples.groupby([\"feature\", \"stat\"])[\"accuracy\"].mean().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die Voraussagekraft steigt wie zu erwarten an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_accuracies(results_many_samples, \"Mehr Daten heben die Genauigkeit und senken die Streuung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Diese Diagramme sehen nett aus, aber die durchschnittlich erzielte Genauigkeit lässt sich daran beispielsweise nicht gut ablesen. Dafür bieten sich boxplots an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_accuracies(results_many_samples, \"Mehr Daten heben die Genauigkeit und senken die Streuung\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "## Klassifikation mit einem neuronalen Netz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun tauschen wir den Bayes-Klassifikator gegen ein neuronales Netz aus. Wir nutzen dazu die deep learning-Bibliothek [keras](https://keras.io) und wählen ein einfache Netz-Architektur: mehrere sequentiell aufeinander folgenden Schichten von Neuronen. Die dafür benötigte Funktionalität stellt keras mit der Modell-Klasse `Sequential` und dem Untermodul `layers`bereit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Die Architektur des neuronalen Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als *Eingabe* für unser Netz verwenden wir dieselben Daten wie zuvor: jede Rede wird durch einen Vektor dargestellt, dessen i-te Komponente eine statistische Größe für das i-te Token des Gesamtvokabulars enthält. Dieser Eingabevektor wird von einer *ersten Schicht von Neuronen* aufgegriffen. Jedes Neuron dieser Schicht \n",
    "  * wichtet die Komponenten des Eingabevektors mit Gewichten, die das Neuron in der Trainingsphase optimiert,\n",
    "  * bildet anschließend die Summe der gewichteten Komponeten und \n",
    "  * wendet auf die Summe eine _Aktivierungsfunktion_ an, hier [`relu`](https://de.wikipedia.org/wiki/Rectifier_(neuronale_Netzwerke)). \n",
    "\n",
    "Die Werte dieser Aktivierungsfunktionen bilden den Ausgabevektor der ersten Schicht. Die Ausgabe der ersten Schicht dient als Eingabe für *eine zweite Schicht* von Neuronen, die nach demselben Prinzip ihrerseits einen Ausgabevektor bildet. Solche dicht vernetzten Schichten können wir nun munter aufeinanderstapeln.\n",
    "\n",
    "Als *Ausgabe* soll uns das Netz sagen, von welchem Politiker die Rede stammt. Dafür  verwenden wir eine Schicht mit genau soviel Neuronen, wie Politiker auftreten: das i-te Neuron gibt uns die vom Netz vermutete Wahrscheinlichkeit dafür an, dass die Rede von dem i-ten Politiker stammt. Diese Wahrscheinlichkeiten müssen sich zu 1 aufsummieren. Das sichern wir mit Hilfe der Aktivierungsfunktion [softmax](https://de.wikipedia.org/wiki/Softmax-Funktion).\n",
    "\n",
    "Den Aufbau des Netzes übernimmt die folgende Funktion `build_seq_model`. Ihre Parameter sind\n",
    "- `x_dim` - die Dimension des Eingabevektors (Größe des Vokabulars),\n",
    "- `y_dim` - die Dimension des Ausgabevektors (Anzahl der Politiker) und\n",
    "- `architecture` - die Anzahl der Neuronen von der ersten bis zur vorletzten Schicht (beziehungsweise die [Dropout-Rate](https://de.wikipedia.org/wiki/Dropout_(k%C3%BCnstliches_neuronales_Netz)), die erst später relevant wird).\n",
    "\n",
    "Bevor wir das Netz trainieren können, muss es noch kompiliert werden. Dazu ist eine Loss-Funktion anzugeben, welche Keras während des Trainings mit dem angegebenen Optimierer zu minimieren versucht. Da die Ausgabe des Netzes eine Wahrscheinlichkeitsverteilung für die gegebenen Politiker ist, wählen wir hier die [Kreuzentropie](https://de.wikipedia.org/wiki/Kreuzentropie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def build_seq_model(x_dim, y_dim, architecture):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(architecture[0], input_shape=(x_dim,),\n",
    "                           activation='relu'))\n",
    "    for s in architecture[1:]:\n",
    "        if s >= 1:\n",
    "            model.add(layers.Dense(s, activation='relu'))\n",
    "        else:\n",
    "            # dieser Fall wird erst später genutzt, erstmal ignorieren!\n",
    "            model.add(layers.Dropout(rate=s))\n",
    "    model.add(layers.Dense(y_dim, activation=\"softmax\"))\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Training und Test des Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Bevor wir das Netz trainieren können, müssen wir uns noch über die Labels Gedanken machen: Während der Bayes-Klassifikator erwartet hat, dass wir die Politiker durchnummerieren, benötigt unser neuronales Netz für Label dasselbe Format, was es auch ausgibt: Wahrscheinlichkeitsverteilungen. Genauer gesagt müssen wir unsere Politiker [one-hot-Kodieren](https://de.wikipedia.org/wiki/1-aus-n-Code): ist die Rede vom i-ten Politiker, so ist das zugehörige Label der Vektor, der an der i-ten Stelle eine 1 und für alle anderen Politiker eine 0 enthält. Das geht mit folgendem `numpy`-Trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def one_hot_encode(y, nr_classes):\n",
    "    return np.eye(nr_classes)[y]\n",
    "\n",
    "one_hot_encode([0,8,1,5], 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun können wir das Netz trainieren. Dazu verwenden wir das Gegenstück zu der obigen Funktion `train_and_test`.\n",
    "Weil das Training ein ganzes Stück länger dauert als beim Bayes-Klassifikator, konzentrieren wir uns aber auf ein Feature (\"token\") und eine Statistik (\"bof\").\n",
    "\n",
    "Erst werden die Daten und Labels für das Training und den Test gesampelt und aufgeteilt, dann die Labels wie beschrieben kodiert und anschließend das Netz gebaut und trainiert. Die Testdaten und -labels werden hier schon übergeben - damit können wir später den Lernprozess besser auswerten. \n",
    "Die Parameter `epochs` und `batch_size` steuern die Dauer und Granularität des Lernprozesses. \n",
    "\n",
    "Das Training liefert ein `History`-Objekt mit Informationen über den Verlauf des Trainings zurück, mehr dazu gleich. \n",
    "Die vom Modell getroffenen Vorhersagen sind Wahrscheinlichkeitsverteilungen. Mit der numpy-Funktion `argmax` bestimmen wir für solch eine Wahrscheinlichkeitsverteilung den Index mit dem größten Wert - das ist dann der von unserem Netz vermutetete Redner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.7\n",
    "EPOCHS = 10\n",
    "FEATURE = \"tokens\"\n",
    "STAT = \"bow\"\n",
    "\n",
    "def train_and_test_net(model, data, labels, epochs): \n",
    "    train_data, train_labels, test_data, test_labels = split(data, labels, SPLIT_RATIO)\n",
    "    nr_classes = np.max(labels) + 1\n",
    "    train_labels_ohe = one_hot_encode(train_labels, nr_classes)\n",
    "    test_labels_ohe = one_hot_encode(test_labels, nr_classes)\n",
    "    history = model.fit(train_data, train_labels_ohe,\n",
    "                        validation_data=(test_data, test_labels_ohe),\n",
    "                        epochs=epochs)\n",
    "    prediction_probas = model.predict(test_data)\n",
    "    predictions = np.argmax(prediction_probas, axis=1)\n",
    "    accuracy = np.mean(predictions == test_labels)\n",
    "    confusion = confusion_matrix(test_labels, predictions)\n",
    "    return (accuracy, confusion, history)\n",
    "\n",
    "def train_and_test_dense_net(num_samples, architecture, epochs=EPOCHS):\n",
    "    sampled_df = sample_speeches(df, num_samples)\n",
    "    labels = sampled_df[\"person\"].astype(\"category\").cat.codes\n",
    "    data = dict_to_sparse(sampled_df[FEATURE + \"_\" + STAT], len(INDEX[FEATURE]))\n",
    "    model = build_seq_model(data.shape[1], np.max(labels) + 1, architecture)\n",
    "    return train_and_test_net(model, data, labels, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun sind wir bereit für den ersten Testlauf!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ARCHITECTURE = [256, 32]\n",
    "\n",
    "(accuracy, confusion, history) = train_and_test_dense_net(50, ARCHITECTURE)\n",
    "print(\"Genauigkeit: \", accuracy)\n",
    "_ = sns.heatmap(confusion).set(title=\"'Confusion'-Matrix für die Klassifikation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Das zusätzliche History-Objekt enthält Informationen über den Verlauf des Trainings, die wir wie folgt visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    loss_dF = pd.DataFrame({\"Trainings-Loss\": history.history[\"loss\"],\n",
    "                            \"Test-Loss\": history.history[\"val_loss\"]})\n",
    "    loss_dF.plot.line().set(xlabel=\"Epoche\", ylabel=\"Loss\", title=\"Trainingsverlauf\")\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Das Bild zeigt den Verlauf der Loss-Funktion nach jeder Epoche des Trainings. Genauer sehen wir\n",
    "die Kreuzentropie zwischen den Vorhersagen und den tatsächlichen one-hot-kodierten Labels\n",
    "- für die Trainingsdaten (`loss`) und\n",
    "- für die Testdaten (`val_loss`).\n",
    "\n",
    "Wir sehen, dass die Loss-Funktion  sehr schnell erfolgreich minimiert wurde - aber nur auf den Trainingsdaten! Das Netz kann das Gelernte offenbar schlecht auf die Testdaten übertragen. Sinkt die Genauigkeit der Vorhersage mit fortschreitendem Training sogar wieder, so spricht man von [Überanpassung/Overfitting](https://de.wikipedia.org/wiki/%C3%9Cberanpassung). Diese tritt stets dann ein, wenn man zu wenig oder nicht repräsentative Trainingsdaten hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Mit Dropout die Genauigkeit verbessern &mdash; das Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wie können wir die Leistung des Modells verbessern? Ein gutes Mittel lautet [Dropout](https://de.wikipedia.org/wiki/Dropout_(k%C3%BCnstliches_neuronales_Netz)) &mdash; bei jedem Lernschritt wird ein Teil der Neuronen \"ausgeblendet\". In `keras` erreicht man dies durch zusätzliche _Dropout-Schichten_. Diese werden zwischen die gewöhnlichen Schichten eingefügt und erhalten als Parameter die _Dropout-Rate_, die angibt, welcher Prozentsatz der Neuronen in der vorherigen Schicht \"ausgeblendet\" werden sollen. Unsere Routine `build_seq_model` fügt immer dann eine Dropout-Schicht ein, wenn der Größen-Parameter für die aktuelle Schicht kleiner 1 ist.\n",
    "\n",
    "Ob dies aber wirklich die Ergebnisse verbessert, können wir nicht an einem Test-Durchlauf ablesen: wie beim Bayes-Klassifikator wird auch hier die Genauigkeit stark von der zufälligen Auswahl der Test- und Trainingsdaten abhängen. Deswegen führen wir ähnlich wie zuvor ein systematisches Experiment durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def neural_experiment(train_and_test_fn, num_samples, rounds, architectures, epochs):\n",
    "    losses = dict()\n",
    "    accuracies = dict()\n",
    "    for architecture in architectures:\n",
    "        print(\"Testdurchlauf für Netzarchitektur \", architecture)\n",
    "        for rnd in range(rounds):\n",
    "            print(\"Runde \", rnd + 1, \"/\", rounds)\n",
    "            (accuracy, _, history) = train_and_test_fn(num_samples, \n",
    "                                                       architecture=architecture, \n",
    "                                                       epochs=epochs)\n",
    "            losses[(str(architecture), \"Trainings-Loss\", rnd)] = history.history[\"loss\"]\n",
    "            losses[(str(architecture), \"Test-Loss\", rnd)] = history.history[\"val_loss\"]\n",
    "            accuracies[(str(architecture), rnd)] = accuracy\n",
    "    return (pd.DataFrame(accuracies, index=[0]),\n",
    "            pd.DataFrame(losses, index=range(0, epochs)).rename_axis(\"Epoche\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Das folgende Experiment kann je nach Rechner eine Weile dauern. Zeit für einen Kaffee! Oder einfach das Experiment erstmal überspringen &mdash; wir haben es dafür auskommentiert, einmal bereits durchgeführt und die Ergebnisse abgespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_PATH = \"experiments\"\n",
    "\n",
    "ARCHITECTURES = [[256, 64, 16], [256, 64, 0.5, 16],\n",
    "                 [1024, 64], [1024, 0.5, 64]]\n",
    "\n",
    "def save_results(accuracies, losses, name):\n",
    "    accuracies.to_pickle(os.path.join(EXPERIMENT_PATH, name + \"_accuracies.pickle\"))\n",
    "    losses.to_pickle(os.path.join(EXPERIMENT_PATH, name + \"_losses.pickle\"))\n",
    "   \n",
    "#(accuracies, losses) = neural_experiment(train_and_test_dense_net, \n",
    "#                                         num_samples=50, rounds=10, \n",
    "#                                         architectures=ARCHITECTURES, epochs=10)\n",
    "\n",
    "# save_results(accuracies, losses, \"dense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir lesen nun die gerade berechneten oder vorher schon abgespeicherten Ergebnisse wieder ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def load_results(name):\n",
    "    accuracies = pd.read_pickle(os.path.join(EXPERIMENT_PATH, \n",
    "                                             name + \"_accuracies.pickle\"))\n",
    "    losses = pd.read_pickle(os.path.join(EXPERIMENT_PATH, \n",
    "                                                 name + \"_losses.pickle\"))\n",
    "    return (accuracies, losses)\n",
    "\n",
    "accuracies, losses = load_results(\"dense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die folgende Grafik zeigt, wie für jede Netzwerk-Architektur die Genauigkeit in den jeweiligen Durchläufen streut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_net_accuracies(accuracies):\n",
    "    acc_long = accuracies.melt(var_name=[\"Netzarchitektur\", \"Runde\"], value_name=\"Genauigkeit\")\n",
    "    sns.boxplot(y=\"Netzarchitektur\", x=\"Genauigkeit\", data=acc_long)\n",
    "    \n",
    "plot_net_accuracies(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Hier zeigt sich, dass Dropout die Ergebnisse jeseil deutlich verbessert.\n",
    "\n",
    "Zum Schluss betrachten wir noch, wie sich Trainings-Loss und Validierungs-Loss während des Lernprozesses entwickelt haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    loss_long = losses.reset_index().melt(id_vars=\"Epoche\", \n",
    "                                          var_name=[\"Netzarchitektur\", \"\",\n",
    "                                                    \"Runde\"],\n",
    "                                          value_name=\"Loss\")\n",
    "    sns.relplot(x=\"Epoche\", hue=\"\", col=\"Netzarchitektur\", col_wrap=2, \n",
    "                y=\"Loss\", data=loss_long, kind=\"line\")\n",
    "    \n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "## Wort-Einbettungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "### Einführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "Bisher haben wir für die Klassifikation nur statistische Informationen über die Token, Wortstämme beziehungsweise named entitites genutzt. Wo beziehungsweise in welcher Reihenfolge diese in der Rede auftraten, spielte keine Rolle. Durch unsere statistische Brille betrachtet könnten wir also die beiden Sätze\n",
    "    \n",
    "    \"Ich bin Gargamel und hasse Papa Schlumpf.\"\n",
    "    \n",
    "und\n",
    "\n",
    "    \"Ich bin Papa Schlumpf und hasse Gargamel.\"\n",
    "    \n",
    "nicht unterscheiden! Für manche Anwendungen von NLP ist das in Ordnung, aber für manche muss man den Text als eine _Folge_ von Token behandeln. \n",
    "Dabei müssen wir die Token, die bisher Zeichenketten sind, durch Zahlen beziehungsweise Vektoren darstellen. Die Frage ist, wie?\n",
    "\n",
    "1. Ein erster Ansatz wäre: _einfach durchnummerieren_. Das Problem dabei: (fast) alle Algorithmen würden Token mit nahe beieinander liegenden Nummern als ähnlich ansehen.\n",
    "2. Eine bessere Idee: _one-hot-kodieren_, wie wir das bereits mit den Labeln gemacht haben. \n",
    "Das Problem hierbei: für jedes Token erhalten wir einen Vektor mit hunderttausenden oder mehr Komponenten  - je nachdem, wie groß unser Vokabular ist. Bei solchen Eingaben versagen alle bekannten Algorithmen (siehe [Fluch der hohen Dimensionen](https://de.wikipedia.org/wiki/Fluch_der_Dimensionalit%C3%A4t)).\n",
    "3. Die übliche Lösung: wir verwenden eine _spezielle dichte Einbettung_, englisch _word embedding_, die jedem Wort einen Vektor zuordnet, der mehrere Hundert Komponenten hat, aber nicht hunderttausend oder mehr wie im Ansatz 2.\n",
    "\n",
    "Doch woher kann man solch eine dichte Einbettung nehmen? Die wichtigsten Algorithmen, die solche Einbettungen aus einem riesigen Textvorrat (wie etwa Wikipedia-Seiten) berechnen, sind [word2vec](https://en.wikipedia.org/wiki/Word2vec) und [gloVe](https://nlp.stanford.edu/projects/glove/). Für die englische Sprache ist solch eine Einbettung in [spacy](https://www.spacy.io) verfügbar, für Deutsch und sehr viele weitere Sprachen zum Beispiel in [fasttext](https://fasttext.cc). Wir greifen hier auf eine [Wort-Einbettung](https://devmount.github.io/GermanWordEmbeddings/) von [Andreas Müller](https://github.com/devmount) zurück, die er mit Hilfe der NLP-Bibliothek [gensim](https://radimrehurek.com/gensim/) erstellt hat. Da die Datei der Wort-Vektoren _sehr_ groß ist (704 MB), haben wir eine Teilmenge für die von uns benötigten Worten extrahiert und bereits abgespeichert. Wer möchte, kann die gesamte Originaldatei herunterladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import gensim\n",
    "import gzip\n",
    "\n",
    "WV_URL = \"http://cloud.devmount.de/d2bc5672c523b086/german.model\"\n",
    "WV_REDUCED_URL = \"https://bootcamp.codecentric.ai/data/german.model.reduced\"\n",
    "WV_FILE = \"german.model\"\n",
    "WV_PATH = os.path.join(DATA_PATH, WV_FILE)\n",
    "WV_REDUCED_PATH = WV_PATH + \".reduced\"\n",
    "WV_DIM = 300\n",
    "\n",
    "def load_embedding(reduced=True): \n",
    "    if reduced:\n",
    "        if not os.path.isfile(WV_REDUCED_PATH):\n",
    "            urllib.request.urlretrieve(WV_REDUCED_URL, WV_REDUCED_PATH)\n",
    "        return gensim.models.KeyedVectors.load(WV_REDUCED_PATH)\n",
    "    else:\n",
    "        if not os.path.isfile(WV_PATH):\n",
    "            urllib.request.urlretrieve(WV_URL, WV_PATH)\n",
    "        return gensim.models.KeyedVectors.load_word2vec_format(WV_PATH, binary=True)\n",
    "            \n",
    "w2v = load_embedding(reduced=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Was ist an Wort-Einbettungen so spannend und verblüffend? Sowohl der Abstand als auch die Addition und Subtraktion von Vektoren sind erstaunlich \"sinnvoll\". So kann man mit Wort-Einbettungen Tabu spielen! _Achtung:_ für `w2v` müssen die Buchstaben `ä`, `ö`, `ü`, `ß` durch `ae`, `oe`, `ue`, `ss` ersetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TABU_AUFGABEN = [[\"Deutschland\", \"Kontinent\"],\n",
    "                 [\"Jahreszeit\", \"Schnee\", \"Sommer\"],\n",
    "                 [\"Stuhl\", \"Gemuetlich\"],\n",
    "                 [\"Fest\", \"Winter\", \"Christkind\"],\n",
    "                ]\n",
    "\n",
    "def tabu(aufgaben=TABU_AUFGABEN):\n",
    "    for woerter in aufgaben: \n",
    "        antwort, sicherheit = w2v.most_similar(woerter, topn=1)[0]\n",
    "        print(\" + \".join(woerter), \"=>\",  antwort, \"(%f)\" % sicherheit)\n",
    "        \n",
    "tabu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die Antworten sind gar nicht so schlecht, oder?\n",
    "\n",
    "Interessant ist auch, sich eine Projektion mehrerer Wortvektoren in der Ebene anzuschauen. Eine geeignete Projektionsebene wählen wir mit Hilfe einer [Hauptkomponentenanalyse/principal component analysis](https://de.wikipedia.org/wiki/Hauptkomponentenanalyse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "WORTLISTE = [\"Mutter\", \"Tochter\", \"Sohn\", \"Opa\", \"Oma\", \n",
    "             \"Vater\", \"Grossvater\", \"Grossmutter\"]\n",
    "\n",
    "def plot_word_vectors(woerter=WORTLISTE):\n",
    "    pca = PCA(n_components=2)\n",
    "    coords = pca.fit_transform(np.vstack([w2v[w] for w in woerter]))\n",
    "    plt.axis((-1.5,2.5,-1.5,1.5))\n",
    "    plt.scatter(coords[:,0], coords[:,1])\n",
    "    for (i, w) in enumerate(woerter):\n",
    "        plt.annotate(w, xy=coords[i] + (0.05,0.05))\n",
    "    plt.title(\"Wortvektoren kodieren Semantik\")\n",
    "        \n",
    "plot_word_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Klassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun wollen wir schließlich noch eine Klassifikation mit Hilfe der Worteinbettungen trainieren. \n",
    "\n",
    "Dazu nehmen wir von jeder Rede die ersten `SAMPLE_LEN` Token, schreiben deren jeweilige Wortvektoren nebeneinander in ein 2-dimensionales numpy-Array und verwenden dieses Array als Darstellung der Rede für die Klassifikation. Dabei müssen wir bei den Token Umlaute ersetzen und die Wörter groß schreiben. Zur Kontrolle notieren wir auch, für welche Token `w2v` _keinen_ Wortvektor enthält. Doch bevor wir anfangen, werfen wir erstmal einen Blick darauf, wieviel Token unsere abgeschnittenen Reden eigentlich enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[\"tokens_len\"] = df.tokens.map(len)\n",
    "_ = sns.boxenplot(x=\"tokens_len\", y=\"person\", data=df).set(title=\"Länge der abgeschnittenen Reden in Token\", xlabel=\"\", ylabel=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir begrenzen die Redenlängen nun auf 150 Token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_LEN = 150\n",
    "WV_DIM = 300\n",
    "\n",
    "def replace(text):\n",
    "    pattern = [('ä', 'ae'), ('ö', 'oe'), ('ü', 'ue'), ('Ä', 'Ae'), ('Ö', 'Oe'), ('Ü', 'Ue'), ('ß', 'ss')]\n",
    "    for (old, new) in pattern: \n",
    "        text = text.replace(old, new)\n",
    "    return text[0].upper() + text[1:]\n",
    "\n",
    "\n",
    "def wvseq(speech, sample_len=SAMPLE_LEN):\n",
    "    vectors = np.zeros((sample_len, WV_DIM))\n",
    "    (j, t, found, missed) = (0, 0, [], []) \n",
    "    for (j,token) in zip(range(0,sample_len), speech):\n",
    "        tok = replace(token)\n",
    "        if tok in w2v:\n",
    "            vectors[j] = w2v[tok]\n",
    "            found.append(tok)\n",
    "        else:\n",
    "            missed.append(tok)\n",
    "    return (vectors, found, missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "speech = [\"Hallo\", \"das\", \"ist\", \"interessanterweise\", \"XYZ\", \"unklar\", \"!\"]\n",
    "(vectors, found, missed) = wvseq(speech, len(speech))\n",
    "missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun wenden wir diese Vorverarbeitung auf unsere Reden an und prüfen, welcher Prozentsatz an Token in `w2v` fehlt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "wvs = df.lemmata.map(wvseq)\n",
    "df[\"word_vectors\"] = wvs.map(lambda x: x[0])\n",
    "df[\"missed_prop\"] = wvs.map(lambda x: len(x[2])/(len(x[1])+len(x[2])))\n",
    "_ = sns.boxplot(df.missed_prop).set(xlabel=\"\", title=\"Anteil fehlender Token in den Reden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wir verwenden nun die vorverarbeiteten Daten zur Klassifikation mit Hilfe eines neuronalen Netzwerks, das im Wesentlichen aus Faltungs-, Batchnormalisierungs- und Poolings-Schichten bestent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "def build_conv_model(architecture, y_dim):\n",
    "    model = Sequential()\n",
    "    (c1, k1, p1, m1, c2, k2, p2, m2) = architecture\n",
    "    model.add(layers.Conv1D(c1, k1, input_shape=(SAMPLE_LEN, WV_DIM)))\n",
    "    model.add(layers.BatchNormalization(momentum=m1))\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "    model.add(layers.AveragePooling1D(p1))\n",
    "    model.add(layers.Conv1D(c2, k2))\n",
    "    model.add(layers.BatchNormalization(momentum=m2))\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "    model.add(layers.AveragePooling1D(p2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(y_dim, activation=\"softmax\"))\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_test_conv_net(num_samples, architecture, epochs=EPOCHS):\n",
    "    sampled_df = sample_speeches(df, num_samples)\n",
    "    labels = sampled_df[\"person\"].astype(\"category\").cat.codes\n",
    "    data = np.stack(sampled_df.word_vectors.values)\n",
    "    model = build_conv_model(architecture, np.max(labels) + 1)\n",
    "    return train_and_test_net(model, data, labels, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Bereit für einen Testlauf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ARCHITECTURE = (64, 8, 8, 0.8, 64, 8, 8, 0.8)\n",
    "\n",
    "(acc, cm, hist) = train_and_test_conv_net(50, architecture=ARCHITECTURE)     \n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Das Ergebnis ist nicht so berauschend, wenn man bedenkt, dass wir vorher schon eine durchschnittliche Genauigkeit von an die 70 Prozent erzielt hatten. Andererseits verwenden wir nun von jeder Rede nur die ersten 150 Token (beziehungsweise rund 100 Token, wenn man bedenkt, dass einige nicht in `w2v` enthalten sind). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Das Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Ähnlich wie vorher werfen wir noch einen Blick auf die Streuung der Genauigkeit, die sich aus der zufälligen Auswahl an Trainings- und Testreden ergibt, und testen verschiedene Netz-Parameter. Das Experiment kann wieder eine Weile dauern. Wir haben es deswegen auskommentiert, bereits einmal durchgeführt und die Ergebnisse abgespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ARCHITECTURES = [(64, 8, 8, 0.99, 64, 8, 8, 0.99), (64, 8, 8, 0.8, 64, 8, 8, 0.8), \n",
    "                 (64, 8, 8, 0.6, 64, 8, 8, 0.6), (128, 8, 8, 0.7, 64, 8, 8, 0.7)]\n",
    "\n",
    "#(accuracies, losses) = neural_experiment(train_and_test_conv_net, num_samples=50, rounds=10, \n",
    "#                                         architectures=ARCHITECTURES, epochs=15)\n",
    "#save_results(accuracies, losses, \"conv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Hier die Genauigkeiten und die Lernverläufe noch einmal visualisiert. Wie zuvor beobachten wir eine Überanpassung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "accuracies, losses = load_results(\"conv\")\n",
    "plot_net_accuracies(accuracies)\n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Bei unseren Experimenten haben die fortgeschritteneren Techniken &mdash; also die neuronalen Netze und die Worteinbettungen &mdash; erstmal nicht die Vorhersagegenauigkeit geliefert, die wir uns vielleicht erhofft hatten. Aber wir sind auch von einer _sehr kleinen Datenmenge_ ausgegangen. Wenn Du Lust und Zeit hast und mit dem ungekürzten Datensatz experimentierst, wirst Du feststellen, dass sich die Vorhersagegenauigkeit wesentlich verbessert!\n",
    "\n",
    "_Wir hoffen, dass Dir das Experimentieren Spaß gemacht hat!  \n",
    "Komm bald wieder zum codecentric.ai Bootcamp!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "nlp_basics.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "399.533px",
    "width": "431px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
