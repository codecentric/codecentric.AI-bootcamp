{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# codecentric.AI Bootcamp &mdash; NLP / Übungsaufgaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "*Hallo und herzlich willkommen zum codecentric.AI bootcamp!*\n",
    "\n",
    "Im Tutorial haben wir bereits eine Anwendung von NLP vorgestellt. Nun wollen wir Dich in einem\n",
    "praktischen Teil dazu einladen, selbst ein paar typische NLP-Techniken auszuprobieren und zu implementieren. Dabei erfährst Du,\n",
    "\n",
    "1. wie elegant sich N-Gramme in Python extrahieren lassen,\n",
    "2. wieviel Stopp-Wörter Politiker verwenden,\n",
    "3. wie man in wenigen Zeilen \"Wer fällt aus der Reihe?\" programmiert,\n",
    "4. was sich hinter dem tf-idf-Maß verbirgt und wie man scikit-learn-Transformer verwendet.\n",
    "\n",
    "Damit es gleich losgehen kann, benötigen wir die folgenden Bibliotheken.\n",
    "\n",
    "*Viel Spaß!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Aufgabe 1: Von einzelnen Wörtern zu Wortgruppen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Als erste Möglichkeit, einen Text zu analysieren, haben wir uns dessen *einzelne Wörter* beziehungsweise dessen bag of words angeschaut. Oft ist es hilfreich, zusätzlich *Wortgruppen* anzuschauen, etwa alle auftretenden Wortpaare. Für den Beispielsatz\n",
    "\n",
    "> \"Fischers Fritz fischt frische Fische\"\n",
    "\n",
    "wären das also\n",
    "\n",
    "> \"Fischers Fritz\", \"Fritz fischt\", \"fischt frische\", \"frische Fische\".\n",
    "\n",
    "Allgemein bezeichnet man als [N-Gramm](https://de.wikipedia.org/wiki/N-Gramm) ein Tupel von N aufeinander folgenden Token. Ist N=2 wie im Beispiel, so spricht man von _Bigrammen_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (a)  Bigramme extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Schreibe eine Funktion `bigrams`, die eine Liste `words` von Wörtern als Eingabe nimmt und die Menge aller auftretenden Bigramme zurückgibt. Tipp: Verwende `zip`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def bigrams(words):\n",
    "    return set(zip(words[:-1], words[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Fischers', 'Fritz'),\n",
       " ('Fritz', 'fischt'),\n",
       " ('fischt', 'frische'),\n",
       " ('frische', 'Fische')}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORDS = [\"Fischers\", \"Fritz\", \"fischt\", \"frische\", \"Fische\"]\n",
    "bigrams(WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (b) N-Gramme extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Schreibe nun eine Funktion `ngrams`, die eine Liste `words` von Wörtern und eine Zahl `n` erwartet und die Menge aller n-Gramme zurückgibt. Tipp: mit `zip(*iters)` kann man `zip` auf eine ganze Liste `iters` von `Iterable`s anwenden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def ngrams(words, n):\n",
    "    l = len(words) - n + 1\n",
    "    return set(zip(*[words[i:l + i] for i in range(0,n)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Fischers', 'Fritz', 'fischt'),\n",
       " ('Fritz', 'fischt', 'frische'),\n",
       " ('fischt', 'frische', 'Fische')}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(WORDS,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Aufgabe 2: Wer verwendet die meisten Stopp-Wörter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Ein üblicher Arbeitsschritt bei NLP ist die Entfernung sogenannter [Stopp-Wörter](https://de.wikipedia.org/wiki/Stoppwort), die häufig auftreten, aber für den Anwendungsfall keine wesentliche Informationen beinhalten. Wir testen, wieviel Stopp-Wörter in den Reden auftauchen, die wir im Tutorial klassifiziert hatten. Dazu lesen wir die Reden nochmal ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "ANALYSIS_FILE = \"speeches.pickle\"\n",
    "ANALYSIS_PATH = os.path.join(DATA_PATH, ANALYSIS_FILE)\n",
    "\n",
    "df = pd.read_pickle(ANALYSIS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (a) Stopp-Wörter filtern "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Eine Liste von 231 Stopp-Wörtern ist in [NLTK](https://www.nltk.org/) enthalten. Wir haben diese Stopp-Wörter zeilenweise in der Datei `data/stopwords_nltk.txt` abgespeichert. Schreibe\n",
    "\n",
    "- eine Funktion `read_stopwords`, die die Liste der Stopp-Wörter einliest und als Menge zurückgibt,\n",
    "- eine Funktion  `filter_words`, die aus einer Liste `words` alle Wörter ausfiltert, die in `stopwords` enthalten sind, und die bereinigte Liste zurückgibt. \n",
    "\n",
    "Tips:\n",
    "\n",
    "- Öffne die Datei der Stopwörter mit der Option `encoding=\"utf-8\"`.\n",
    "- Benutze gegebenenfalls die Methode [rstrip](https://docs.python.org/3/library/stdtypes.html), um Zeilenumbrüche von eingelesenen Strings zu entfernen.\n",
    "- `filter_words` sollte ein Einzeiler sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Teste Deine Funktionen wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "EXAMPLE = [\"Das\", \"ist\", \"ein\", \"ganz\", \"und\", \"gar\", \"normaler\", \"Satz\", \"mit\", \"vielen\", \"Wörtern\"]\n",
    "stopwords = read_stopwords()\n",
    "filter_words(EXAMPLE, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (b) Anwendung auf die Reden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Füge nun dem pandas-DataFrame `df` eine Spalte `filtered_tokens` hinzu, die für jede Rede die Liste der Token enthält, aus denen die Stopp-Wörter entfernt wurden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Füge als Nächstes `df` eine Spalte `stop_percentage` hinzu, die für jede Rede angibt, wieviel Prozent der Rede Stopp-Wörter waren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (d) Stopp-Wort-Anteile visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Plotte abschließend den prozentualen Anteil an Stopp-Wörtern in den Redern nach Politikern gruppiert in einem geeigneten. Wodurch fallen Kanzler auf?\n",
    "\n",
    "Tipp: Verwende [kategorielle Plot-Funktionen](https://seaborn.pydata.org/tutorial/categorical.html) von seaborn oder schau im Tutorial nach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Aufgabe 3: Wer fällt aus der Reihe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Word embeddings kann man nicht nur für ernsthaftes NLP verwenden, sondern auch, um Spaß zu haben. Im Tutorial hatten wir schon ein kleines Tabu-Spiel programmiert. Nun wollen wir in wenigen Zeilen eine Funktion `find_the_odd` schreiben, die aus einer Liste von Wörtern den Ausreißer herausfindet &mdash; das Wort, das aus der Reihe fällt. Zum Beispiel also\n",
    "\n",
    "> Fruehling, Sommer, Abend, Herbst\n",
    "> => Abend\n",
    "\n",
    "\n",
    "Dazu benötigen wir wieder die Wortvektoren aus der [Arbeit](https://devmount.github.io/GermanWordEmbeddings/) von [Andreas Müller](https://github.com/devmount)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "WV_PATH = os.path.join(\"data\", \"german.model.reduced\")\n",
    "\n",
    "w2v = gensim.models.KeyedVectors.load(WV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (a) Ähnlichkeitsmatrix berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Die Wortvektoren sind nun in als gensim-`KeyedVectors`-Objekt `w2v` verfügbar. Die Methode `similarity` berechnet zu gegebenen Wörtern nun deren [Cosinus-Ähnlichkeit](https://de.wikipedia.org/wiki/Kosinus-%C3%84hnlichkeit): 0 heißt keine Ähnlichkeit, 1 heißt Übereinstimmung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "w2v.similarity(\"Mensch\", \"Maschine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Schreibe nun eine Funktion `similarity_matrix`, die eine Liste `words` von Wörtern als Eingabe nimmt und ein 2-dimensionales numpy-Array zurückliefert, dessen Eintrag an der Stelle `[i,j]` die Cosinus-Ähnlichkeit zwischen den Wörtern `words[i]` und `words[j]` ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def similarity_matrix(words):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Tipp: Mit Hilfe der numpy-Funktion [asarray](https://docs.scipy.org/doc/numpy/reference/generated/numpy.asarray.html) und List-Komprehensionen geht das in zwei Zeilen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (b) Ausreißer finden "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Um den Ausreißer in einer Liste `words` von Wörtern zu finden, können wir nun die Matrix `similarity_matrix(words)` nehmen und die Einträge jeder Zeile aufsummieren: die Zeile mit der kleinsten Summe entspricht dem Ausreißer. Schreibe eine Funktion `find_the_odd`, welche dies umsetzt. Hilfreich sind dabei die numpy-Funktionen [np.sum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html) und [np.argmin](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def find_the_odd(words):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (c) Probier es aus! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Teste nun Deine Funktion an folgenden Beispielen und denk Dir selber welche aus! Beachte dabei, dass alle Wörter groß geschrieben und die Umlaute `ä`, `ö`, `ü` sowie `ß` als `ae`, `oe`, `ue` beziehungsweise `ss`umgeschrieben werden müssen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ODDS = [[\"Fruehling\", \"Sommer\", \"Abend\", \"Herbst\"],\n",
    "        [\"Fruehstueck\", \"Oma\", \"Abendbrot\", \"Mittagessen\"],\n",
    "        [\"Rot\", \"Gruen\", \"Blau\", \"Maler\"],\n",
    "        [\"Hund\", \"Mensch\", \"Hase\", \"Katze\"],\n",
    "        [\"Tochter\", \"Mutter\", \"Grossmutter\", \"Cousine\"]\n",
    "    ]\n",
    "\n",
    "for odd_set in ODDS:\n",
    "    print(\", \".join(odd_set), \" => \", find_the_odd(odd_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Aufgabe 4: Klassifikation mit dem tf-idf-Maß und scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In dem Tutorial  haben wir zwei statistische Größen für die Klassifikation der Reden genutzt: *welche* Token, Grundformen beziehungsweise Begriffe auftauchen, zusammengefasst in einer *bag of words*, und *wie oft* jedes Token et cetera in jeder Rede auftaucht. In dieser Aufgabe lernen wir weitere wichtige Größen kennen:\n",
    "\n",
    "- die *Vorkommenshäufigkeit*, welche im Englischen *term frequency* (*tf*) genannt wird,\n",
    "- die *inverse Dokumentvorkommenshäufigkeit*, englisch *inverse document frequency* (*idf*),\n",
    "- das kombinierte [tf-idf-Maß](https://de.wikipedia.org/wiki/Tf-idf-Ma%C3%9F).\n",
    "\n",
    "Diese messen jeweils\n",
    "\n",
    "- wie \"wichtig\" das Token für die Rede ist &mdash; je häufiger, desto wichtiger,\n",
    "- wie \"spezifisch\" das Token für die gesamte Redensammlung ist &mdash; je häufiger, desto unwichtiger,\n",
    "- eine Kombination beider Aspekte: ein Token ist umso bedeutender, je öfter es in der Rede auftaucht und je seltener in anderen.\n",
    "\n",
    "Mit Hilfe dieser Größen wollen wir nun die Reden, die wir bereits im Tutorial angeschaut haben, noch einmal klassifizieren. Dazu laden wir erstmal den aufbereiteten Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "ANALYSIS_FILE = \"speeches.pickle\"\n",
    "\n",
    "df = pd.read_pickle(os.path.join(DATA_PATH, ANALYSIS_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (a) Vorverarbeitung mit scikit-learn-Transformern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "[scikit-learn](https://scikit-learn.org) bietet zahlreiche Hilfsmittel für die Vorverarbeitung von Daten, unter anderem die Klasse [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Bestimme damit die Tfidf-Statistiken der Reden in folgenden Schritten:\n",
    "\n",
    "1. Bestimme die Menge aller Token, die in den Reden auftreten, und nennen diese Menge `vocab`;\n",
    "2. erzeuge mit `TfIdfVectorizer(vocabulary=vocab)` ein Objekt `vectorizer`;\n",
    "3. bilde für jede Rede aus der Liste der Token eine Zeichenkette, indem die Token mit Leerzeichen verknüpft werden;\n",
    "4. wende die Methode `vectorizer.fit_transform` auf das erhaltene `Iterable` von Zeichenketten an und nenne das Ergebnis `tfidf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Wenn alles geklappt hat, können wir uns die wichtigsten Terme einer Rede wie folgt anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "REDE = 13 # Index der Rede\n",
    "termsAndTfidfs = list(zip(vocab, tfidf.toarray()[REDE]))\n",
    "termsAndTfidfs.sort(reverse=True, key=lambda termAndTfidf: termAndTfidf[1])\n",
    "termsAndTfidfs[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (b) Klassifikation mit dem tf-idf-Maß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Nun kannst Du mit den gewonnenen tf-idf-Daten einen Bayes-Klassifizierer trainieren. Die wesentlichen Schritte sind wie im Tutorial\n",
    "\n",
    "1. Zerlegung der Daten in Trainings- und Testdaten &mdash; dafür bietet scikit-learn die Routine [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html);\n",
    "2. das eigentliche Training mit Hilfe von [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) und der Methode `fit` sowie\n",
    "3. die Beurteilung des Trainings-Erfolgs mit Hilfe der Testdaten mit Hilfe der Methode `predict` und der Funktion [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Es stellt sich wahrscheinlich heraus, dass keine zufriedenstellende Qualität erreicht wird. Natürlich könnte oder sollte man das wie im Tutorial durch wiederholte Tests genauer prüfen. Der Grund ist schnell gefunden: pro Redner haben wir im Schnitt _zu wenig Reden_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "nlp_basics-aufgaben.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "130.533px",
    "width": "250px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
